{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metalearned Neural Memory with local memory updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy, sys, logging, json, time, random, os, string, pickle, re\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modules.Vocabulary import Vocab\n",
    "from modules.LoadTrainSave import save_model, load_model, Teacher\n",
    "from modules.TransformerComponents import Transformer\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNMp -The Memory Controller \n",
    "\n",
    "Imagine that an LSTM is the agent. LSTMs takes in a current state x_t, the previous hidden state h_t-1 and output the current hidden state h_t\n",
    "\n",
    "Now imagine that our LSTM has a storage center for memory besides h_t, the \n",
    "`neural memory network` (NMN), and at each step it receives a current state x_t, the previous hidden state h_t-1 and a read out from the NMN v_t-1. \n",
    "\n",
    "Each x_t has shape (batch size, n_units), each readout vector v_t-1 has shape `(batch size, self.n_in_mem)`\n",
    "\n",
    "Since the LSTM is initialized `torch.nn.LSTMCell(input_size, hidden_size, bias=True)` We concatenate the input x_t with the memory readout vector v_t-1\n",
    "\n",
    "`self.lstm_l1 = nn.LSTMCell(n_units + n_in_mem, n_units)`\n",
    "\n",
    "In the paper this LSTM controller is \n",
    "\n",
    "$$h_t = LSTM(x_t, v^{r}_{t-1}, h_{t-1})$$\n",
    "\n",
    "The first time that the MNMp forward pass is used, the initial memory readout vector - self.v, and LSTM controller hidden states - self.h_lstm and self.c_lstm is initialized as zero vectors, it is then updated thereafter with each subsequent forward pass x_t.\n",
    "\n",
    "In PyTorch the LSTMCell takes an input and a hidden state (hidden state, cell state) tuple, so this mathematical term appears as \n",
    "\n",
    "`self.h_lstm, self.c_lstm = self.lstm_l1(torch.cat([x, self.v], dim=1), (self.h_lstm, self.c_lstm))`\n",
    "\n",
    "The hidden state output of the LSTM controller is placed through an affine transformation and tanh non-linearity to produce a large vector that is then separated into the interaction vectors.\n",
    "\n",
    "$$[k^{r}_{t,1} . . . k^{r}_{t,H}; k^{w}_{t,1} . . .  k^{w}_{t,H}; v^{w}_{t,1}. . . v^{w}_{t,H}; B_{t}] = tanh(W_v h_t + b_v)$$\n",
    "\n",
    "`int_vecs = torch.tanh(self.heads_l2(self.h_lstm))`\n",
    "\n",
    "These interaction terms contain value vectors to write to memory, key vectors that if seen again in the future are meant to retreive these memorized values, and also key vectors to retreive memorys for immediate use.\n",
    "\n",
    "Leaveing out the number of heads, each step of the LSTM agent generates two keys and one value. A read key to retreive memory, and a write key and write value to store memory.\n",
    "\n",
    "For each sample in the batch, there are H sets of 3 vectors, the read key, write key and write value, plus one rate vector beta_.  \n",
    "\n",
    "\n",
    "A new read value is read from the memory function using the read key in the `v_r = self.memfunc.read(k_r)` step. \n",
    "\n",
    "The rate vector is projected to a scalar \"rate\", called beta in the range 0 to 1. The operation\n",
    "\n",
    "`n_k_v = n_k_v.view(n_k_v.shape[0], self.n_heads, -1).contiguous()`\n",
    "\n",
    "The rate vector is separated from the other interaction vectors in this step, \n",
    "this portion of the interaction vectors n_k_v consists of the write key-value pairs,\n",
    "and the read key. \n",
    "\n",
    "`beta_, n_k_v = torch.split(int_vecs,[self.dim_hidden,self.dim_hidden*self.n_heads*3],dim=1)`\n",
    "\n",
    "The shape of the n_k_v undergoes these shape changes \n",
    "\n",
    "(batchsize, dim_hidden*n_heads*3)->(batchsize,n_heads,dim_hidden*3)\n",
    "\n",
    "`n_k_v = n_k_v.view(n_k_v.shape[0], self.n_heads, -1).contiguous()`\n",
    "        \n",
    "`k_w, v_w, k_r = torch.chunk(n_k_v, 3, dim=2)`\n",
    "\n",
    "Now each of these tensors k_w, v_w, k_r is of shape (batchsize, n_heads, dim_hidden)\n",
    "\n",
    "k_w, v_w are passed to the memory update function along with the rate scalar beta. How the Memory is updated is explained in the FFMemory section\n",
    "\n",
    "`re_const_loss, re_const_loss_init = self.memfunc.update(k_w, v_w, beta_rate=beta)`\n",
    "\n",
    "The controller hidden state is updated using both the previous hidden state and the read value produced from the memory\n",
    "\n",
    "`h_lstm = self.read_out(torch.cat([self.h_lstm, self.v_r], dim=1))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNMp(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_hidden, n_heads = 4):\n",
    "        \n",
    "        \"\"\" dim_hidden is the hidden size of the LSTM controller,\n",
    "            the Memory Network, and the interaction vectors\n",
    "            n_heads is the number of interaction heads \"\"\"\n",
    "\n",
    "        super(MNMp, self).__init__()\n",
    "        \n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.control = nn.LSTMCell(dim_hidden*2, dim_hidden)\n",
    "        \n",
    "        dim_concat_interact = dim_hidden*n_heads*3 + dim_hidden\n",
    "        self.interaction = nn.Linear(dim_hidden, dim_concat_interact)\n",
    "        self.memfunc = FFMemoryLearned(dim_hidden)\n",
    "        self.kv_rate = nn.Linear(dim_hidden, 1)\n",
    "        self.read_out = nn.Linear(dim_hidden+dim_hidden, dim_hidden)\n",
    "        \n",
    "        self.v_r = torch.nn.Parameter(torch.zeros((1, self.dim_hidden)))\n",
    "        self.h_lstm = torch.nn.Parameter(torch.zeros((1, self.dim_hidden)))\n",
    "        self.c_lstm = torch.nn.Parameter(torch.zeros((1, self.dim_hidden)))\n",
    "        \n",
    "    def repeat_v_h_c(self, batch_size):\n",
    "\n",
    "            v_r = self.v_r.repeat(batch_size,1)\n",
    "            h_lstm = self.h_lstm.repeat(batch_size,1)\n",
    "            c_lstm = self.c_lstm.repeat(batch_size,1)\n",
    "            \n",
    "            if next(self.parameters()).is_cuda:\n",
    "                self.v_r = self.v_r.cuda()\n",
    "                self.h_lstm = self.h_lstm.cuda()\n",
    "                self.c_lstm = self.c_lstm.cuda()\n",
    "                                         \n",
    "            return v_r, h_lstm, c_lstm\n",
    "            \n",
    "    def forward(self, context_vector):\n",
    "        \n",
    "        \"\"\" the input context_vector must have shape (batch_size, emb_dim) because it will be \n",
    "        concatenated with self.v_r of the same shape \"\"\"\n",
    "\n",
    "        v_r, h_lstm, c_lstm = self.repeat_v_h_c(context_vector.shape[0])\n",
    "        \n",
    "        context_vector = context_vector.squeeze(1) # (batch_size, 1, emb_dim) -> (batch_size, 1, emb_dim)\n",
    "        \n",
    "        # combine the current state x with the previous memory v_r, hidden and cell states\n",
    "        h_lstm, c_lstm = self.control(torch.cat([context_vector, v_r], dim=1), (h_lstm, c_lstm))\n",
    "        \n",
    "        int_vecs = torch.tanh(self.interaction(h_lstm))\n",
    "        \n",
    "        beta_, n_k_v = torch.split(int_vecs, \n",
    "                                   [self.dim_hidden,self.dim_hidden*self.n_heads*3],\n",
    "                                   dim=1)  \n",
    "        \n",
    "        beta = torch.sigmoid(self.kv_rate(beta_)) #beta shape (batch_size,1)\n",
    "        \n",
    "        n_k_v = n_k_v.view(n_k_v.shape[0], self.n_heads, -1).contiguous()\n",
    "        \n",
    "        # Separate out our 3 interaction terms \n",
    "        k_w, v_w, k_r = torch.chunk(n_k_v, 3, dim=2)\n",
    "        \n",
    "        # use the first 2 terms to bind the write keya nd write value\n",
    "        reconst_loss, reconst_loss_init = self.memfunc.update(k_w, v_w, beta_rate=beta)\n",
    "        \n",
    "        # use the last term to retreive memory \n",
    "        v_r = self.memfunc.read(k_r)\n",
    "        \n",
    "        # combine the retreived memory with the hidden state into the context vector\n",
    "        context_vector = self.read_out(torch.cat([h_lstm, v_r], dim=1))\n",
    "\n",
    "        return context_vector.unsqueeze(1), reconst_loss, reconst_loss_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Learning Rule\n",
    "\n",
    "The perceptron learning rule is used to do fast gradient free updates to the neural memory network. This perceptron learning rule is explained well here: http://hagan.okstate.edu/4_Perceptron.pdf \n",
    "\n",
    "The perceptron learning rule allows you to change the weights of a linear transformation matrix in such a way as to nudge it's output closer to or away from a desired target or vector of targets, without having to calculate a mean squared error, and perform backpropagation. \n",
    "\n",
    "Consider the some layer of a feed forward neural network, activations \"a\" = relu(Wx+b).\n",
    "\n",
    "\"a\" might have several dimensions, but the first dimension of \"a\" is only dependant on x and the first row of W. So if you consider W to be a single row and \"a\" to be a single activation, then without loss of generality, what applies for this row applies to the entire matrix W and the entire vector a. \n",
    "\n",
    "if `W = [-1, 1]` and `b = [-1]`, you can see by plotting `W[x_1, x_2] + b = 0` on a horizontal `x_1` by vertical `x_2` axis that this is a line with slope `1` that intersects the `x_1` axis at `-1`. \n",
    "\n",
    "<img src = 'files/grapher1.gif' width=400 height =400>\n",
    "\n",
    "This is the decision boundary because at this boundary `Wx+b=0` and on either side of the boundary you are either positive or negative.  The point `p1 = x_1, x_2 = 2, 1` would be evaluated as `-2` if you plugged it into `Wx+b` and so would be zero after an activation function `f(a) = max(0,a)`. You can tell which side of the line above is the negative and positive regime by seeing which direction the vector `[-1, 1]` points. Moving `-1` along `x_1` means one step to the left, and `1` step upwards along `x_2` draws an arrow that point up and to the left. So it seems the point `[2,1]` is on the wrong side of this line if you want it to be positive. That direction that vector `W` points in, is the direction towards the side of the decision boundary that is positive in `Wx`. The opposite side of the boundary is negative in `Wx`. \n",
    "\n",
    "Suppose without calculating the gradient you want to change `W` such that the point `p1 = x_1, x_2 = 2, 1` will be classified as positive. if you add `p1` to `W` you get `W = [1, 2]`, and if you add the error to the bias, ie `error = target - activation = 1 - 0 = 1`, you get `error + b_old = -1 + 1 = b_new = 0`\n",
    "\n",
    "<img src = 'files/grapher2.gif' width=400 height =400>\n",
    "\n",
    "The new line points up and to the right and will now classify p1 correctly. `Wx + b = 2+2+1 = 5`. Notice that b also changes in the correct direction, if you didnt change b, the resulting decision boundary `[1, 2]x - 1 = 0` would have been a negatively sloped line that intersects the `x_1` axis at `x_1 = 1`, whereas now that `b = 1`, the intersect is at `0`. This update has not only rotated the vector `W` to point towards `p1` but shifted the decision boundary downwards pulling the boundary away from `p1` so that `p1` is farther within the region of positivity and therefore more positive.\n",
    "\n",
    "The perceptron learning rule formula is as follows:\n",
    "\n",
    "$$ W_t := W_{t-1} + (target - activation) \\otimes input^{T} $$\n",
    "\n",
    "$$ b_t = b_{t-1} + (target - activation) $$\n",
    "\n",
    "`t` is the update iteration timestep. The circle with an x inside is the outer product, this fits with the shape of W since if the activation is m-dimensional, and input is n-dimensional, the W is shape mxn and the bias is shape mx1. Therefore the the update to W must be the same shape as W which is the shape you would get if you performed an outer product between (target - activation) x (input^T) = (update) which has shape (mx1)(1xn) = (mxn)\n",
    "\n",
    " In our example (target - activation) x (input^T) = (1 - 0) x `[[2],[1]]^T` = `[2,1]`\n",
    "\n",
    "For us the target vector is not some kind of supervised label, instead it is a target that the `backward feedback prediction functions (BFPF)` learns to come up with the target activations `z'` for all the layers of the `neural memory network` (NMN) in such a way as to be the target int he update rule to bind the vector to vector mapping of key value pairs.\n",
    "\n",
    "Our memory is stored in a multi-layer feed forward NMN\n",
    "\n",
    "$$ v^{r}_{t,i} = {NMN}(k^{r}_{t,i})$$\n",
    "\n",
    "$$ z'^{l} = {BFPF}^{l}(v^{w}_{}t) $$\n",
    "\n",
    "v^w is the memorywe want to remember, ie write to memory. k^r is the thought that occurs in real time that prompts us to retrieve v^r. \n",
    "\n",
    "v^r is the retreived memory for timestep t and i-th head, i-th because there might be several memories we want to retreive, t because we might be performing this retrieval for each step in a task.\n",
    "\n",
    "Lets re-write the above perceptron rule in a format that is analoguous to the layer by layer update rule in [Metalearned Neural Memory](https://arxiv.org/abs/1907.09720)\n",
    "\n",
    "$$ W_t := W_{t-1} - (activation - target) \\otimes x^{T} $$\n",
    "\n",
    "M^l is the weight matrix for layer l of the NMN, t is the new matrix after update, t-1 is the old matrix before update, B is a learning rate, z^l is the current activation for layer l prior to update, z'^l is the target activation for layer l. z^{l-1} are activations of the previous layer\n",
    "\n",
    "$$ M^{l}_{t} :=  M^{l}_{t-1} - B^{l}_{t}(z^{l} - z'^{l}) {z^{l-1}}^{T} $$\n",
    "\n",
    "These concepts are reflected in `class FFMemoryLearned(nn.Module):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFMemoryLearned(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    This is a minimal 3 layer feed forward neural net with\n",
    "    methods for meta-learned, aka, fast local updates\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, dim_hidden):\n",
    "        \n",
    "        super(FFMemoryLearned, self).__init__()\n",
    "        \n",
    "        # In the future you can use the get_clones function in TransformerComponents\n",
    "        # that uses the nn.ModuleList object to make several copies of the \n",
    "        self.l1 = nn.Linear(dim_hidden, dim_hidden).weight.data\n",
    "        self.l2 = nn.Linear(dim_hidden, dim_hidden).weight.data\n",
    "        self.l3 = nn.Linear(dim_hidden, dim_hidden).weight.data\n",
    "        \n",
    "        self.Ws = [self.l1, self.l2, self.l3]\n",
    "        self.Ws_temp = None\n",
    "        \n",
    "        # the 3 in  3*dim_hidden just reflects the fact that we define 3 layers above\n",
    "        self.expected_activation = nn.Linear(dim_hidden, 3*dim_hidden)\n",
    "\n",
    "    def if_cuda(self):\n",
    "        \n",
    "        if next(self.parameters()).is_cuda:\n",
    "            self.l1 = self.l1.cuda()\n",
    "            self.l2 = self.l2.cuda()\n",
    "            self.l3 = self.l3.cuda()\n",
    "            \n",
    "        self.Ws = [self.l1, self.l2, self.l3]\n",
    "\n",
    "    def detach_mem(self):\n",
    "        \n",
    "        ''' \n",
    "        In order to enable automatic differentiation, PyTorch keeps track of all operations involving\n",
    "        tensors for which the gradient may need to be computed (i.e., require_grad is True). \n",
    "        The operations are recorded as a directed graph. The detach() method constructs a new view on a \n",
    "        tensor which is declared not to need gradients, \n",
    "        i.e., it is to be excluded from further tracking of operations, and therefore \n",
    "        the sub-graph involving this view is not recorded.\n",
    "        .detach() doesnt implicitly create a copy of the tensor, so when the tensor is modified later, \n",
    "        itâ€™s updating the tensor on the upstream side of .detach()\n",
    "        '''\n",
    "        \n",
    "        for W_l in self.Ws:\n",
    "            \n",
    "            W_l.detach()\n",
    "            \n",
    "        self.Ws_temp = None\n",
    "\n",
    "    def forward(self, key):\n",
    "        \n",
    "        '''\n",
    "        The forward pass through the neural memory network (NMN).\n",
    "        This is used both in the read (retreive) and write (update) operations to memory\n",
    "        \n",
    "        Input:\n",
    "            key - type (torch.Tensor) shape (batch_size, num_heads, dim_hidden): \n",
    "                this is either the read_key or write_key input to the NMN\n",
    "        Output:\n",
    "            value - (torch.Tensor) shape (batch_size, num_heads, dim_hidden):\n",
    "                the output, aka last layer activation, from the NMN \n",
    "            activations - (list) of length (num layers - 1): \n",
    "                the layer activations, except the last one, in an ordered list\n",
    "                each of shape [batch_size, num_heads, dim_hidden])\n",
    "        '''\n",
    "        \n",
    "        self.if_cuda()\n",
    "        \n",
    "        # is reset at beginning of every new sequence \n",
    "        # If self.Ws_temp is not None, thats because we have just done a batched update\n",
    "        # and self.Ws_temp has a matrix for each batch sample \n",
    "        if self.Ws_temp is None: \n",
    "            \n",
    "          self.Ws_temp = []\n",
    "        \n",
    "          for W_l in self.Ws:\n",
    "                \n",
    "            # replicate weights of each layer across each batch sample\n",
    "            # (emb_dim, emb_dim) -> (batch_size, emb_dim, emb_dim)\n",
    "            \n",
    "            W = W_l.unsqueeze(dim=0).expand((key.shape[0], W_l.shape[0], W_l.shape[1]))\n",
    "\n",
    "            self.Ws_temp.append(W)\n",
    "            \n",
    "        activations = [] # hidden activations except the last  \n",
    "        \n",
    "        a = key\n",
    "        \n",
    "        for W_l in self.Ws_temp[:-1]:\n",
    "\n",
    "            a = torch.matmul(a, W_l.transpose(1,2))\n",
    "            a = torch.sigmoid(a)\n",
    "            activations.append(a)\n",
    "        \n",
    "        value = torch.matmul(a, self.Ws_temp[-1].transpose(1,2))\n",
    "\n",
    "        return value, activations\n",
    "\n",
    "    def mse_loss(self, y_pred, y):\n",
    "        'mean squared error'\n",
    "        \n",
    "        diff = y_pred - y\n",
    "        diff = diff.view(-1)\n",
    "        mse = diff.dot(diff)/diff.size()[0]\n",
    "        \n",
    "        return mse\n",
    "\n",
    "    def read(self, k, weights=None, avg=True):\n",
    "        'read from memory'\n",
    "        \n",
    "        v, h_acts = self.forward(k)\n",
    "\n",
    "        if weights is not None:\n",
    "          v *= weights\n",
    "        if avg:\n",
    "          v = v.mean(dim=1)\n",
    "        \n",
    "        return v\n",
    "\n",
    "    def update(self, k_w, v_w, beta_rate=0.1):  \n",
    "        \n",
    "        '''\n",
    "        This function performs a single update to the neural memory network (NMN).\n",
    "        The single update implements the perceptron learning rule (PLR) whereby the network\n",
    "        is updated to closer approximate the target activations. After successful learning\n",
    "        of the target activations and learning rates (beta_rate), the single update PLR can\n",
    "        sufficiently map the key vector (k_w) to the value vector (v_w) to return the appropriate \n",
    "        tokens in conversation.\n",
    "        \n",
    "        PLR is analogous to the local layer update in target propagation\n",
    "        The forward function is run twice, once before and once after the local update\n",
    "        \n",
    "        inputs:\n",
    "            k_w, v_w - shape [batch_size, num_heads, dim_hidden]\n",
    "            beta_rate - shape [batch_size, 1, 1]\n",
    "        outputs:\n",
    "            reconst_loss - the reconstruction loss after PLR update\n",
    "            reconst_loss_init - the reconstruction loss before PLR update\n",
    "        '''\n",
    "\n",
    "        v_w_approx, activations = self.forward(k_w)\n",
    "        \n",
    "        v_w_approx = v_w_approx.contiguous()\n",
    "        v_w = v_w.contiguous()\n",
    "        \n",
    "        # before local update, see how well the this key and value are already bound\n",
    "        reconst_loss_init=self.mse_loss(v_w_approx.view(-1,v_w_approx.shape[2]),v_w.view(-1,v_w.shape[2]))\n",
    "        \n",
    "        # backward feedback prediction functions predict the expected activations of each NMN layer\n",
    "        # v_w.view(-1, v_w.shape[2]) performs shape change \n",
    "        # [batch_size, num_heads, emb_dim] -> [batch_size*num_heads, emb_dim]\n",
    "\n",
    "        # z_pr is the target activations for each layer\n",
    "        z_pr = self.expected_activation(v_w.view(-1, v_w.shape[2]))\n",
    "        # output z_pr is of shape [batch_size*num_heads, emb_dim*num_layers]\n",
    "        z_pr = z_pr.view(v_w.shape[0], v_w.shape[1], -1)\n",
    "        # output z_pr is of shape [batch_size, num_heads, emb_dim*num_layers]\n",
    "        z_pr = torch.chunk(z_pr, 3, dim=2) # result is a tuple of length 3\n",
    "        # each element in z_pr is shape [batch_size, num_heads, emb_dim]\n",
    "        \n",
    "        if len(beta_rate.shape) < 3:\n",
    "            beta_rate = beta_rate.unsqueeze(1)\n",
    "\n",
    "        # z2 and z1 are offset by 1\n",
    "        z2 = activations + [v_w_approx] # z2 are the initial output activations of the 3 layers\n",
    "        z1 = [k_w] + activations # z1 are the initial input activations of the 3 layers\n",
    "        \n",
    "        # Ws_t is a list of the updated weights of the NMN \n",
    "        Ws_t = []\n",
    "        \n",
    "        # iterates through the 4 lists in reverse order as x_, applying the perceptron learning rule \n",
    "        # diff is shape (batchsize, heads, dim_hidden)\n",
    "        for W_l, z2_, z1_, z_pr in list(zip(self.Ws_temp, z2, z1, z_pr)):\n",
    "            \n",
    "            \n",
    "            diff = z2_ - z_pr # activation minus target_activation, \n",
    "            diff = diff*beta_rate.expand(diff.shape) # diff.shape (batch_size, heads, emb_dim)\n",
    "            diff = diff/(diff.shape[1]*diff.shape[2]) #re-normalize prevents explosion with larger dims\n",
    "            # transpose(1,2) switches the emb_dim and heads dims so you get:\n",
    "            #(batch_size,emb_dim,heads)x(batch_size,heads,emb_dim)=(batch_size, emb_dim, emb_dim)\n",
    "            W_update = torch.matmul(diff.transpose(1,2), z1_)\n",
    "            \n",
    "            W_l = W_l - W_update \n",
    "            \n",
    "            Ws_t.append(W_l)\n",
    "        \n",
    "        self.Ws_temp[:] = Ws_t\n",
    "        \n",
    "        # Run memory forward again after memory update to see how well\n",
    "        # the v_w has been reconstructed as v_w_approx, use mse to\n",
    "        # quantify the quality of the key value binding as the reconst_loss\n",
    "        \n",
    "        v_w_approx, activations = self.forward(k_w)\n",
    "        \n",
    "        reconst_loss = self.mse_loss(v_w_approx.view(-1, v_w_approx.shape[2]), v_w.view(-1, v_w.shape[2]))\n",
    "        \n",
    "        return reconst_loss, reconst_loss_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product increases as the dimensions get larger. A fix is to introduce a scaling factor, and was proposed in Transformers [Attention Is All You Need](https://arxiv.org/abs/1706.03762). This function also has no learnable parameters. Above we scale down diff in proportion to the number of heads and number of hidden dimensions so that when we perform the matrix multiply with the previous activations, this dot product does not blow up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bot(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim, n_layers, heads, dropout, vocab):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.sos_tok = torch.LongTensor([[self.vocab.word2index[\"<SOS>\"]]]) \n",
    "        self.eos_tok = torch.LongTensor([[self.vocab.word2index[\"<EOS>\"]]]) \n",
    "        \n",
    "        self.encodeInput = Transformer(emb_dim, n_layers, heads, dropout)\n",
    "        self.encodeEncoding = Transformer(emb_dim, n_layers, heads, dropout)\n",
    "        self.decodeEncoding = Transformer(emb_dim, n_layers, heads, dropout)\n",
    "\n",
    "        self.mnm = MNMp(emb_dim, heads)\n",
    "        \n",
    "        self.context_vec = None\n",
    "        \n",
    "    def memory_utils(self, batch_size):\n",
    "\n",
    "        if self.context_vec is None:\n",
    "            cntxt_seq_len = 1\n",
    "            self.context_vec = torch.randn(batch_size, cntxt_seq_len, self.emb_dim)\n",
    "            \n",
    "        if self.context_vec.shape[0] > batch_size:\n",
    "            self.context_vec = self.context_vec[0,:,:]\n",
    "            \n",
    "        if self.context_vec.shape[0] < batch_size:\n",
    "            self.context_vec = self.context_vec[0,:,:].repeat(batch_size, 1, 1)\n",
    "    \n",
    "        self.context_vec = self.context_vec.detach()\n",
    "        self.mnm.memfunc.detach_mem()\n",
    "        \n",
    "    def forward(self, in_toks, in_mask, out_toks, out_mask):\n",
    "        \n",
    "        self.memory_utils(batch_size = in_toks.shape[0])\n",
    "        \n",
    "        in_vecs = self.vocab.embedding(in_toks)\n",
    "        out_vec = self.vocab.embedding(out_toks)\n",
    "\n",
    "        self.context_vec, rcl, rcli = self.mnm(self.context_vec)\n",
    "        encin_vec = self.encodeInput(in_vecs, in_mask, self.context_vec, None)\n",
    "        self.context_vec = self.encodeEncoding(self.context_vec, None, encin_vec, None)\n",
    "        \n",
    "        dout = self.decodeEncoding(out_vec, out_mask, encin_vec, in_mask)\n",
    "        \n",
    "        return dout, rcl, rcli\n",
    "    \n",
    "    def teacher_forcing(self, src, trg):\n",
    "        \n",
    "        self.train()\n",
    "        trg_start = torch.cat((self.sos_tok.repeat(trg.shape[0],1), trg),dim=1)\n",
    "        trg_end = torch.cat((trg, self.eos_tok.repeat(trg.shape[0],1)),dim=1)\n",
    "        src_mask = (src != self.vocab.word2index[\"<PAD>\"]).unsqueeze(-2)\n",
    "        trg_mask = (trg_end != self.vocab.word2index[\"<PAD>\"]).unsqueeze(-2)\n",
    "        \n",
    "        seq_len = trg_start.size(1) \n",
    "        np_mask = np.triu(np.ones((1,seq_len,seq_len)),k=1).astype('uint8')\n",
    "        np_mask =  torch.from_numpy(np_mask) == 0\n",
    "        \n",
    "        if trg.is_cuda:\n",
    "            np_mask = np_mask.cuda()\n",
    "            \n",
    "        trg_mask = trg_mask & np_mask\n",
    "        \n",
    "        out_vecs, rcl, rcli = self.forward(src, src_mask, trg_start, trg_mask)\n",
    "        \n",
    "        return out_vecs, trg_end, rcl, rcli\n",
    "    \n",
    "    def string2string(self, input_string, maxlen = 20):\n",
    "        \n",
    "        self.eval()\n",
    "        in_toks = self.vocab.string2tensor(input_string)\n",
    "        in_vecs = self.vocab.embedding(in_toks)\n",
    "        \n",
    "        self.memory_utils(batch_size=in_toks.shape[0])\n",
    "        \n",
    "        self.context_vec, rcl, rcli = self.mnm(self.context_vec)\n",
    "        encin_vec = self.encodeInput(in_vecs, None, self.context_vec, None)\n",
    "        self.context_vec = self.encodeEncoding(self.context_vec, None, encin_vec, None)\n",
    "        \n",
    "        decode_toks = self.sos_tok\n",
    "        \n",
    "        for pos in range(maxlen):\n",
    "            \n",
    "            decode_vecs = self.vocab.embedding(decode_toks)\n",
    "            dout = self.decodeEncoding(decode_vecs, None, encin_vec, None)\n",
    "            vocabdist = self.vocab.emb2vocab(dout)\n",
    "            next_toks = torch.argmax(vocabdist, dim=2)\n",
    "            decode_toks = torch.cat((decode_toks, next_toks[:,-1].unsqueeze(0)), dim=1) \n",
    "            \n",
    "            if next_toks[:,-1] == self.eos_tok.squeeze(0):\n",
    "                \n",
    "                toks = decode_toks[0][1:-1].data.cpu().numpy()\n",
    "                de_str = ' '.join([self.vocab.index2word[int(tok)] for tok in toks])\n",
    "\n",
    "                return de_str\n",
    "            \n",
    "        toks = decode_toks[0].data.cpu().numpy()\n",
    "        de_str = ' '.join([self.vocab.index2word[tok] for tok in toks])\n",
    "        \n",
    "        return de_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, 'my': 4, 'name': 5, 'is': 6, ',': 7, 'hi': 8, '.': 9, 'what': 10, '?': 11, 'its': 12}\n"
     ]
    }
   ],
   "source": [
    "emb_dim, n_layers, heads, dropout = 16, 2, 3, 0.001\n",
    "\n",
    "vocab = Vocab(emb_dim)\n",
    "model = Bot(emb_dim, n_layers, heads, dropout, vocab)\n",
    "teacher = Teacher(model.vocab)\n",
    "\n",
    "print(model.vocab.word2index)\n",
    "\n",
    "best_acc = 0\n",
    "learning_rate = 0.01 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best...\n",
      "mean accuracy 0.0312 celoss 11.3169 rcloss 0.502653 d_rcloss 0.0119 training progress 0.0 learning rate 0.01\n",
      "New Best...\n",
      "mean accuracy 0.2083 celoss 3.1368 rcloss 3.4e-05 d_rcloss 0.007 training progress 0.0101 learning rate 0.01\n",
      "New Best...\n",
      "mean accuracy 0.2604 celoss 2.4754 rcloss 0.000156 d_rcloss 0.0072 training progress 0.0202 learning rate 0.01\n",
      "New Best...\n",
      "mean accuracy 0.2969 celoss 2.2655 rcloss 0.000191 d_rcloss 0.0088 training progress 0.0303 learning rate 0.01\n",
      "New Best...\n",
      "mean accuracy 0.3104 celoss 2.2141 rcloss 0.000194 d_rcloss 0.0101 training progress 0.0404 learning rate 0.01\n",
      "New Best...\n",
      "mean accuracy 0.316 celoss 2.3824 rcloss 3.8e-05 d_rcloss 0.0113 training progress 0.0505 learning rate 0.01\n",
      "New Best...\n",
      "mean accuracy 0.3274 celoss 2.1217 rcloss 1.2e-05 d_rcloss 0.0138 training progress 0.0606 learning rate 0.01\n",
      "New Best...\n",
      "mean accuracy 0.3698 celoss 2.0265 rcloss 3e-06 d_rcloss 0.0135 training progress 0.0707 learning rate 0.01\n",
      "New Best...\n",
      "mean accuracy 0.3866 celoss 2.0756 rcloss 7.8e-05 d_rcloss 0.009 training progress 0.0808 learning rate 0.01\n",
      "New Best...\n",
      "mean accuracy 0.4188 celoss 1.9154 rcloss 3e-06 d_rcloss 0.0124 training progress 0.0909 learning rate 0.01\n",
      "New Best...\n",
      "mean accuracy 0.4896 celoss 1.9311 rcloss 1.6e-05 d_rcloss 0.012 training progress 0.101 learning rate 0.01\n",
      "New Best...\n",
      "mean accuracy 0.524 celoss 1.9 rcloss 6.3e-05 d_rcloss 0.0143 training progress 0.1111 learning rate 0.01\n",
      "New Best...\n",
      "mean accuracy 0.5583 celoss 1.8983 rcloss 2.3e-05 d_rcloss 0.0105 training progress 0.1212 learning rate 0.01\n",
      "New Best...\n",
      "mean accuracy 0.5958 celoss 1.7651 rcloss 0.000133 d_rcloss 0.0106 training progress 0.1313 learning rate 0.009\n",
      "New Best...\n",
      "mean accuracy 0.6344 celoss 1.591 rcloss 0.000344 d_rcloss 0.0165 training progress 0.1414 learning rate 0.009\n",
      "New Best...\n",
      "mean accuracy 0.6833 celoss 1.4478 rcloss 8.9e-05 d_rcloss 0.0157 training progress 0.1515 learning rate 0.009\n",
      "New Best...\n",
      "mean accuracy 0.7281 celoss 1.3438 rcloss 0.001825 d_rcloss 0.0175 training progress 0.1616 learning rate 0.009\n",
      "New Best...\n",
      "mean accuracy 0.7448 celoss 1.5444 rcloss 0.000146 d_rcloss 0.0215 training progress 0.1717 learning rate 0.0081\n",
      "New Best...\n",
      "mean accuracy 0.7792 celoss 1.2927 rcloss 0.000369 d_rcloss 0.0256 training progress 0.1818 learning rate 0.0081\n",
      "New Best...\n",
      "mean accuracy 0.7896 celoss 1.281 rcloss 0.000119 d_rcloss 0.0245 training progress 0.1919 learning rate 0.0081\n",
      "New Best...\n",
      "mean accuracy 0.8073 celoss 1.0211 rcloss 0.00016 d_rcloss 0.027 training progress 0.202 learning rate 0.0081\n",
      "New Best...\n",
      "mean accuracy 0.8177 celoss 1.3545 rcloss 0.000675 d_rcloss 0.0281 training progress 0.2121 learning rate 0.0081\n",
      "New Best...\n",
      "mean accuracy 0.8354 celoss 1.0175 rcloss 0.000526 d_rcloss 0.0297 training progress 0.2222 learning rate 0.00729\n",
      "New Best...\n",
      "mean accuracy 0.8542 celoss 0.9295 rcloss 0.000434 d_rcloss 0.0309 training progress 0.2323 learning rate 0.00729\n",
      "New Best...\n",
      "mean accuracy 0.875 celoss 0.8578 rcloss 0.000621 d_rcloss 0.0286 training progress 0.2424 learning rate 0.00729\n",
      "New Best...\n",
      "mean accuracy 0.8865 celoss 0.8084 rcloss 0.000193 d_rcloss 0.0297 training progress 0.2525 learning rate 0.00729\n",
      "New Best...\n",
      "mean accuracy 0.899 celoss 0.756 rcloss 0.000127 d_rcloss 0.0317 training progress 0.2626 learning rate 0.00729\n",
      "New Best...\n",
      "mean accuracy 0.9083 celoss 0.8076 rcloss 9e-05 d_rcloss 0.0328 training progress 0.2727 learning rate 0.00729\n",
      "New Best...\n",
      "mean accuracy 0.9115 celoss 1.0995 rcloss 0.002108 d_rcloss 0.0291 training progress 0.2828 learning rate 0.00729\n",
      "New Best...\n",
      "mean accuracy 0.9219 celoss 0.8613 rcloss 0.00026 d_rcloss 0.0325 training progress 0.2929 learning rate 0.006561\n",
      "New Best...\n",
      "mean accuracy 0.9271 celoss 0.6527 rcloss 8.9e-05 d_rcloss 0.0333 training progress 0.303 learning rate 0.006561\n",
      "New Best...\n",
      "mean accuracy 0.9385 celoss 0.8249 rcloss 0.000671 d_rcloss 0.0346 training progress 0.3131 learning rate 0.006561\n",
      "New Best...\n",
      "mean accuracy 0.949 celoss 0.5871 rcloss 8.2e-05 d_rcloss 0.0338 training progress 0.3232 learning rate 0.006561\n",
      "mean accuracy 0.9469 celoss 0.6372 rcloss 0.000257 d_rcloss 0.0361 training progress 0.3333 learning rate 0.006561\n",
      "mean accuracy 0.9479 celoss 0.6093 rcloss 0.000452 d_rcloss 0.0342 training progress 0.3434 learning rate 0.006561\n",
      "New Best...\n",
      "mean accuracy 0.951 celoss 0.5411 rcloss 0.000303 d_rcloss 0.0341 training progress 0.3535 learning rate 0.006561\n",
      "New Best...\n",
      "mean accuracy 0.9521 celoss 0.5001 rcloss 0.000211 d_rcloss 0.0358 training progress 0.3636 learning rate 0.006561\n",
      "New Best...\n",
      "mean accuracy 0.9573 celoss 0.5289 rcloss 0.000492 d_rcloss 0.0341 training progress 0.3737 learning rate 0.006561\n",
      "New Best...\n",
      "mean accuracy 0.9667 celoss 0.4377 rcloss 0.000279 d_rcloss 0.0346 training progress 0.3838 learning rate 0.006561\n",
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, 'my': 4, 'name': 5, 'is': 6, ',': 7, 'hi': 8, '.': 9, 'what': 10, '?': 11, 'its': 12, 'salvador': 13, 'carson': 14, 'melissa': 15, 'force': 16, 'sky': 17, 'adam': 18, 'zen': 19, 'vicki': 20}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17806de20>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_batches = 10000\n",
    "lamda = 4\n",
    "batch_size = 32\n",
    "printevery = 100\n",
    "\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate,betas=(0.9, 0.98),eps=1e-9)\n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min',factor=0.9,patience=100)\n",
    "\n",
    "loss_all_list = []\n",
    "rcloss_all_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "for batch in range(total_batches):\n",
    "    \n",
    "    #print('remember info')\n",
    "    \n",
    "    intro, introtarget, whatmyname, yournameis = teacher.get_batch(batch_size)\n",
    "    \n",
    "    out_vecs, trg_end, rcl, rcli = model.teacher_forcing(intro, introtarget)\n",
    "    \n",
    "    vocab_logits = model.vocab.emb2vocab(out_vecs)\n",
    "    \n",
    "    predictions = vocab_logits.view(-1, vocab_logits.size(-1))\n",
    "    \n",
    "    target = trg_end.view(-1)\n",
    "\n",
    "    batch_loss = F.cross_entropy(predictions, target, \n",
    "                                 ignore_index = model.vocab.word2index[\"<PAD>\"])\n",
    "\n",
    "    reconstruction_loss = lamda*rcl\n",
    "    \n",
    "    ################# Next Part of Conversation ########################\n",
    "    \n",
    "    #print('retreive memory')\n",
    "    \n",
    "    out_vecs, trg_end, rcl, rcli = model.teacher_forcing(whatmyname, yournameis)\n",
    "    \n",
    "    vocab_logits = model.vocab.emb2vocab(out_vecs)\n",
    "\n",
    "    predictions = vocab_logits.view(-1, vocab_logits.size(-1))\n",
    "    \n",
    "    target = trg_end.view(-1)\n",
    "    \n",
    "    acc = accuracy_score(target, torch.argmax(predictions, dim=1))\n",
    "\n",
    "    batch_loss += F.cross_entropy(predictions, target, \n",
    "                                 ignore_index = model.vocab.word2index[\"<PAD>\"])\n",
    "    \n",
    "    reconstruction_loss += lamda*rcl\n",
    "    conversation_loss = batch_loss + reconstruction_loss\n",
    "    \n",
    "    scheduler.step(conversation_loss)\n",
    "    optimizer.zero_grad()\n",
    "    conversation_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if batch % int(printevery + 1) == 0:\n",
    "        \n",
    "        loss_all_list.append(conversation_loss.float().item())\n",
    "        rcloss_all_list.append(reconstruction_loss.float().item())\n",
    "        accuracy_list.append(acc)\n",
    "        mean_accuracy = np.mean(accuracy_list[-10:])\n",
    "        learning_rate = scheduler._last_lr[0]\n",
    "\n",
    "        if mean_accuracy > best_acc:\n",
    "            print('New Best...')\n",
    "            best_acc = mean_accuracy\n",
    "            \n",
    "        print(\n",
    "              \"mean accuracy\", round(mean_accuracy,4), \n",
    "              \"celoss\", round(batch_loss.float().item(),4), \n",
    "              \"rcloss\", round(reconstruction_loss.float().item(),6), \n",
    "              \"d_rcloss\", round((rcli - rcl).float().item(),4),\n",
    "              \"training progress\", round(batch/total_batches,4),\n",
    "              \"learning rate\", round(learning_rate,6),\n",
    "             )\n",
    "            \n",
    "        if mean_accuracy > 0.96:\n",
    "            break\n",
    "\n",
    "print(model.vocab.word2index)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
    "fig.suptitle('Training Curves')\n",
    "ax1.set(xlabel='epochs', ylabel='train loss')\n",
    "ax2.set(xlabel='epochs', ylabel='reconstr loss')\n",
    "ax3.set(xlabel='epochs', ylabel='accuracy')\n",
    "ax1.plot(loss_all_list, label='train loss')\n",
    "ax2.plot(rcloss_all_list, label='reconstrunction loss')\n",
    "ax3.plot(accuracy_list, label='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model.vocab.word2index,open(\"modelstate/word2index_memory.p\",\"wb\"))\n",
    "pickle.dump(model.vocab.index2word,open(\"modelstate/index2word_memory.p\",\"wb\"))\n",
    "pickle.dump(model.vocab.emb2vocab.weight,open(\"modelstate/emb2vocab.weight_memory.p\",\"wb\"))\n",
    "pickle.dump(model.vocab.embedding.weight,open(\"modelstate/embedding.weight_memory.p\",\"wb\"))\n",
    "pickle.dump(model.context_vec,open(\"modelstate/context_vec_memory.p\",\"wb\"))\n",
    "pickle.dump(model.mnm.memfunc.Ws,open(\"modelstate/Ws_memory.p\",\"wb\"))\n",
    "save_model(model,\"modelstate/task_memory.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bot(\n",
       "  (vocab): Vocab(\n",
       "    (embedding): Embedding(4, 16)\n",
       "    (emb2vocab): Linear(in_features=16, out_features=4, bias=False)\n",
       "  )\n",
       "  (encodeInput): Transformer(\n",
       "    (pe): PositionalEncoder(\n",
       "      (dropout): Dropout(p=0.001, inplace=False)\n",
       "    )\n",
       "    (encodelayers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (dropout_1): Dropout(p=0.001, inplace=False)\n",
       "        (attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (k_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (v_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (out): Linear(in_features=15, out_features=16, bias=True)\n",
       "        )\n",
       "        (norm_2): Norm()\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=16, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=16, bias=True)\n",
       "        )\n",
       "        (dropout_2): Dropout(p=0.001, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (dropout_1): Dropout(p=0.001, inplace=False)\n",
       "        (attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (k_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (v_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (out): Linear(in_features=15, out_features=16, bias=True)\n",
       "        )\n",
       "        (norm_2): Norm()\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=16, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=16, bias=True)\n",
       "        )\n",
       "        (dropout_2): Dropout(p=0.001, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (decodelayers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.001, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.001, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.001, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (k_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (v_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (out): Linear(in_features=15, out_features=16, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (k_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (v_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (out): Linear(in_features=15, out_features=16, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=16, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.001, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.001, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.001, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (k_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (v_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (out): Linear(in_features=15, out_features=16, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (k_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (v_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (out): Linear(in_features=15, out_features=16, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=16, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): Norm()\n",
       "  )\n",
       "  (encodeEncoding): Transformer(\n",
       "    (pe): PositionalEncoder(\n",
       "      (dropout): Dropout(p=0.001, inplace=False)\n",
       "    )\n",
       "    (encodelayers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (dropout_1): Dropout(p=0.001, inplace=False)\n",
       "        (attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (k_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (v_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (out): Linear(in_features=15, out_features=16, bias=True)\n",
       "        )\n",
       "        (norm_2): Norm()\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=16, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=16, bias=True)\n",
       "        )\n",
       "        (dropout_2): Dropout(p=0.001, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (dropout_1): Dropout(p=0.001, inplace=False)\n",
       "        (attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (k_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (v_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (out): Linear(in_features=15, out_features=16, bias=True)\n",
       "        )\n",
       "        (norm_2): Norm()\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=16, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=16, bias=True)\n",
       "        )\n",
       "        (dropout_2): Dropout(p=0.001, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (decodelayers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.001, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.001, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.001, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (k_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (v_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (out): Linear(in_features=15, out_features=16, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (k_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (v_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (out): Linear(in_features=15, out_features=16, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=16, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.001, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.001, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.001, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (k_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (v_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (out): Linear(in_features=15, out_features=16, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (k_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (v_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (out): Linear(in_features=15, out_features=16, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=16, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): Norm()\n",
       "  )\n",
       "  (decodeEncoding): Transformer(\n",
       "    (pe): PositionalEncoder(\n",
       "      (dropout): Dropout(p=0.001, inplace=False)\n",
       "    )\n",
       "    (encodelayers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (dropout_1): Dropout(p=0.001, inplace=False)\n",
       "        (attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (k_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (v_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (out): Linear(in_features=15, out_features=16, bias=True)\n",
       "        )\n",
       "        (norm_2): Norm()\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=16, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=16, bias=True)\n",
       "        )\n",
       "        (dropout_2): Dropout(p=0.001, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (dropout_1): Dropout(p=0.001, inplace=False)\n",
       "        (attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (k_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (v_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (out): Linear(in_features=15, out_features=16, bias=True)\n",
       "        )\n",
       "        (norm_2): Norm()\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=16, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=16, bias=True)\n",
       "        )\n",
       "        (dropout_2): Dropout(p=0.001, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (decodelayers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.001, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.001, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.001, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (k_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (v_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (out): Linear(in_features=15, out_features=16, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (k_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (v_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (out): Linear(in_features=15, out_features=16, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=16, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.001, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.001, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.001, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (k_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (v_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (out): Linear(in_features=15, out_features=16, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (k_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (v_linear): Linear(in_features=16, out_features=15, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (out): Linear(in_features=15, out_features=16, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=16, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.001, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): Norm()\n",
       "  )\n",
       "  (mnm): MNMp(\n",
       "    (control): LSTMCell(32, 16)\n",
       "    (interaction): Linear(in_features=16, out_features=160, bias=True)\n",
       "    (memfunc): FFMemoryLearned(\n",
       "      (expected_activation): Linear(in_features=16, out_features=48, bias=True)\n",
       "    )\n",
       "    (kv_rate): Linear(in_features=16, out_features=1, bias=True)\n",
       "    (read_out): Linear(in_features=32, out_features=16, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab.emb2vocab.weight = pickle.load(open(\"modelstate/emb2vocab.weight_memory.p\",\"rb\"))\n",
    "model.vocab.embedding.weight = pickle.load(open(\"modelstate/embedding.weight_memory.p\",\"rb\"))\n",
    "model.vocab.word2index = pickle.load(open(\"modelstate/word2index_memory.p\",\"rb\"))\n",
    "model.vocab.index2word = pickle.load(open(\"modelstate/index2word_memory.p\",\"rb\"))\n",
    "load_model(model,\"modelstate/task_memory.pth\")\n",
    "model.mnm.memfunc.Ws = pickle.load(open(\"modelstate/Ws_memory.p\",\"rb\"))\n",
    "model.context_vec = pickle.load(open(\"modelstate/context_vec_memory.p\",\"rb\"))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > my name is melissa\n",
      " > hi melissa\n",
      " \n",
      " > what is my name?\n",
      " > its melissa\n",
      " \n",
      " > my name is vicki\n",
      " > hi vicki\n",
      " \n",
      " > what is my name?\n",
      " > its vicki\n",
      " \n",
      " > my name is zen\n",
      " > hi zen\n",
      " \n",
      " > what is my name?\n",
      " > its zen\n",
      " \n",
      " > my name is sky\n",
      " > hi sky\n",
      " \n",
      " > what is my name?\n",
      " > its sky\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for tell in [\n",
    "             'my name is melissa', 'what is my name?', \n",
    "             'my name is vicki', 'what is my name?',\n",
    "             'my name is zen', 'what is my name?',\n",
    "             'my name is sky', 'what is my name?',\n",
    "             ]:\n",
    "\n",
    "    print(' > '+ tell)\n",
    "    reply = model.string2string(tell)\n",
    "    print(' > '+ reply)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
