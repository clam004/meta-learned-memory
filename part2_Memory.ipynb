{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metalearned Neural Memory with local memory updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy, sys, logging, json, time, random, os, string, pickle, re\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modules.Vocabulary import Vocab\n",
    "from modules.LoadTrainSave import save_model, load_model, Teacher\n",
    "from modules.TransformerComponents import Transformer\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNMp -The Memory Controller \n",
    "\n",
    "Imagine that an LSTM is the agent. LSTMs takes in a current state x_t, the previous hidden state h_t-1 and output the current hidden state h_t\n",
    "\n",
    "Now imagine that our LSTM has a storage center for memory besides h_t, the \n",
    "`neural memory network` (NMN), and at each step it receives a current state x_t, the previous hidden state h_t-1 and a read out from the NMN v_t-1. \n",
    "\n",
    "Each x_t has shape (batch size, n_units), each readout vector v_t-1 has shape `(batch size, self.n_in_mem)`\n",
    "\n",
    "Since the LSTM is initialized `torch.nn.LSTMCell(input_size, hidden_size, bias=True)` We concatenate the input x_t with the memory readout vector v_t-1\n",
    "\n",
    "`self.lstm_l1 = nn.LSTMCell(n_units + n_in_mem, n_units)`\n",
    "\n",
    "In the paper this LSTM controller is \n",
    "\n",
    "$$h_t = LSTM(x_t, v^{r}_{t-1}, h_{t-1})$$\n",
    "\n",
    "The first time that the MNMp forward pass is used, the initial memory readout vector - self.v, and LSTM controller hidden states - self.h_lstm and self.c_lstm is initialized as zero vectors, it is then updated thereafter with each subsequent forward pass x_t.\n",
    "\n",
    "In PyTorch the LSTMCell takes an input and a hidden state (hidden state, cell state) tuple, so this mathematical term appears as \n",
    "\n",
    "`self.h_lstm, self.c_lstm = self.lstm_l1(torch.cat([x, self.v], dim=1), (self.h_lstm, self.c_lstm))`\n",
    "\n",
    "The hidden state output of the LSTM controller is placed through an affine transformation and tanh non-linearity to produce a large vector that is then separated into the interaction vectors.\n",
    "\n",
    "$$[k^{r}_{t,1} . . . k^{r}_{t,H}; k^{w}_{t,1} . . .  k^{w}_{t,H}; v^{w}_{t,1}. . . v^{w}_{t,H}; B_{t}] = tanh(W_v h_t + b_v)$$\n",
    "\n",
    "`int_vecs = torch.tanh(self.heads_l2(self.h_lstm))`\n",
    "\n",
    "These interaction terms contain value vectors to write to memory, key vectors that if seen again in the future are meant to retreive these memorized values, and also key vectors to retreive memorys for immediate use.\n",
    "\n",
    "Leaveing out the number of heads, each step of the LSTM agent generates two keys and one value. A read key to retreive memory, and a write key and write value to store memory.\n",
    "\n",
    "For each sample in the batch, there are H sets of 3 vectors, the read key, write key and write value, plus one rate vector beta_.  \n",
    "\n",
    "\n",
    "A new read value is read from the memory function using the read key in the `v_r = self.memfunc.read(k_r)` step. \n",
    "\n",
    "The rate vector is projected to a scalar \"rate\", called beta in the range 0 to 1. The operation\n",
    "\n",
    "`n_k_v = n_k_v.view(n_k_v.shape[0], self.n_heads, -1).contiguous()`\n",
    "\n",
    "The rate vector is separated from the other interaction vectors in this step, \n",
    "this portion of the interaction vectors n_k_v consists of the write key-value pairs,\n",
    "and the read key. \n",
    "\n",
    "`beta_, n_k_v = torch.split(int_vecs,[self.dim_hidden,self.dim_hidden*self.n_heads*3],dim=1)`\n",
    "\n",
    "The shape of the n_k_v undergoes these shape changes \n",
    "\n",
    "(batchsize, dim_hidden*n_heads*3)->(batchsize,n_heads,dim_hidden*3)\n",
    "\n",
    "`n_k_v = n_k_v.view(n_k_v.shape[0], self.n_heads, -1).contiguous()`\n",
    "        \n",
    "`k_w, v_w, k_r = torch.chunk(n_k_v, 3, dim=2)`\n",
    "\n",
    "Now each of these tensors k_w, v_w, k_r is of shape (batchsize, n_heads, dim_hidden)\n",
    "\n",
    "k_w, v_w are passed to the memory update function along with the rate scalar beta. How the Memory is updated is explained in the FFMemory section\n",
    "\n",
    "`re_const_loss, re_const_loss_init = self.memfunc.update(k_w, v_w, beta_rate=beta)`\n",
    "\n",
    "The controller hidden state is updated using both the previous hidden state and the read value produced from the memory\n",
    "\n",
    "`h_lstm = self.read_out(torch.cat([self.h_lstm, self.v_r], dim=1))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNMp(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_hidden, n_heads = 4):\n",
    "        \n",
    "        \"\"\" dim_hidden is the hidden size of the LSTM controller,\n",
    "            the Memory Network, and the interaction vectors\n",
    "            n_heads is the number of interaction heads \"\"\"\n",
    "\n",
    "        super(MNMp, self).__init__()\n",
    "        \n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.control = nn.LSTMCell(dim_hidden*2, dim_hidden)\n",
    "        \n",
    "        dim_concat_interact = dim_hidden*n_heads*3 + dim_hidden\n",
    "        self.interaction = nn.Linear(dim_hidden, dim_concat_interact)\n",
    "        self.memfunc = FFMemoryLearned(dim_hidden)\n",
    "        self.kv_rate = nn.Linear(dim_hidden, 1)\n",
    "        self.read_out = nn.Linear(dim_hidden+dim_hidden, dim_hidden)\n",
    "        \n",
    "        self.v_r = torch.nn.Parameter(torch.zeros((1, self.dim_hidden)))\n",
    "        self.h_lstm = torch.nn.Parameter(torch.zeros((1, self.dim_hidden)))\n",
    "        self.c_lstm = torch.nn.Parameter(torch.zeros((1, self.dim_hidden)))\n",
    "        \n",
    "    def repeat_v_h_c(self, batch_size):\n",
    "\n",
    "            v_r = self.v_r.repeat(batch_size,1)\n",
    "            h_lstm = self.h_lstm.repeat(batch_size,1)\n",
    "            c_lstm = self.c_lstm.repeat(batch_size,1)\n",
    "            \n",
    "            if next(self.parameters()).is_cuda:\n",
    "                self.v_r = self.v_r.cuda()\n",
    "                self.h_lstm = self.h_lstm.cuda()\n",
    "                self.c_lstm = self.c_lstm.cuda()\n",
    "                                         \n",
    "            return v_r, h_lstm, c_lstm\n",
    "            \n",
    "    def forward(self, context_vector):\n",
    "        \n",
    "        \"\"\" the input context_vector must have shape (batch_size, emb_dim) because it will be \n",
    "        concatenated with self.v_r of the same shape \"\"\"\n",
    "\n",
    "        v_r, h_lstm, c_lstm = self.repeat_v_h_c(context_vector.shape[0])\n",
    "        \n",
    "        context_vector = context_vector.squeeze(1) # (batch_size, 1, emb_dim) -> (batch_size, 1, emb_dim)\n",
    "        \n",
    "        # combine the current state x with the previous memory v_r, hidden and cell states\n",
    "        h_lstm, c_lstm = self.control(torch.cat([context_vector, v_r], dim=1), (h_lstm, c_lstm))\n",
    "        \n",
    "        int_vecs = torch.tanh(self.interaction(h_lstm))\n",
    "        \n",
    "        beta_, n_k_v = torch.split(int_vecs, \n",
    "                                   [self.dim_hidden,self.dim_hidden*self.n_heads*3],\n",
    "                                   dim=1)  \n",
    "        \n",
    "        beta = torch.sigmoid(self.kv_rate(beta_)) #beta shape (batch_size,1)\n",
    "        \n",
    "        n_k_v = n_k_v.view(n_k_v.shape[0], self.n_heads, -1).contiguous()\n",
    "        \n",
    "        # Separate out our 3 interaction terms \n",
    "        k_w, v_w, k_r = torch.chunk(n_k_v, 3, dim=2)\n",
    "        \n",
    "        # use the first 2 terms to bind the write keya nd write value\n",
    "        reconst_loss, reconst_loss_init = self.memfunc.update(k_w, v_w, beta_rate=beta)\n",
    "        \n",
    "        # use the last term to retreive memory \n",
    "        v_r = self.memfunc.read(k_r)\n",
    "        \n",
    "        # combine the retreived memory with the hidden state into the context vector\n",
    "        context_vector = self.read_out(torch.cat([h_lstm, v_r], dim=1))\n",
    "\n",
    "        return context_vector.unsqueeze(1), reconst_loss, reconst_loss_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Learning Rule\n",
    "\n",
    "The perceptron learning rule is used to do fast gradient free updates to the neural memory network. This perceptron learning rule is explained well here: http://hagan.okstate.edu/4_Perceptron.pdf \n",
    "\n",
    "The perceptron learning rule allows you to change the weights of a linear transformation matrix in such a way as to nudge it's output closer to or away from a desired target or vector of targets, without having to calculate a mean squared error, and perform backpropagation. \n",
    "\n",
    "Consider the some layer of a feed forward neural network, activations \"a\" = relu(Wx+b).\n",
    "\n",
    "\"a\" might have several dimensions, but the first dimension of \"a\" is only dependant on x and the first row of W. So if you consider W to be a single row and \"a\" to be a single activation, then without loss of generality, what applies for this row applies to the entire matrix W and the entire vector a. \n",
    "\n",
    "if `W = [-1, 1]` and `b = [-1]`, you can see by plotting `W[x_1, x_2] + b = 0` on a horizontal `x_1` by vertical `x_2` axis that this is a line with slope `1` that intersects the `x_1` axis at `-1`. \n",
    "\n",
    "<img src = 'files/grapher1.gif' width=400 height =400>\n",
    "\n",
    "This is the decision boundary because at this boundary `Wx+b=0` and on either side of the boundary you are either positive or negative.  The point `p1 = x_1, x_2 = 2, 1` would be evaluated as `-2` if you plugged it into `Wx+b` and so would be zero after an activation function `f(a) = max(0,a)`. You can tell which side of the line above is the negative and positive regime by seeing which direction the vector `[-1, 1]` points. Moving `-1` along `x_1` means one step to the left, and `1` step upwards along `x_2` draws an arrow that point up and to the left. So it seems the point `[2,1]` is on the wrong side of this line if you want it to be positive. That direction that vector `W` points in, is the direction towards the side of the decision boundary that is positive in `Wx`. The opposite side of the boundary is negative in `Wx`. \n",
    "\n",
    "Suppose without calculating the gradient you want to change `W` such that the point `p1 = x_1, x_2 = 2, 1` will be classified as positive. if you add `p1` to `W` you get `W = [1, 2]`, and if you add the error to the bias, ie `error = target - activation = 1 - 0 = 1`, you get `error + b_old = -1 + 1 = b_new = 0`\n",
    "\n",
    "<img src = 'files/grapher2.gif' width=400 height =400>\n",
    "\n",
    "The new line points up and to the right and will now classify p1 correctly. `Wx + b = 2+2+1 = 5`. Notice that b also changes in the correct direction, if you didnt change b, the resulting decision boundary `[1, 2]x - 1 = 0` would have been a negatively sloped line that intersects the `x_1` axis at `x_1 = 1`, whereas now that `b = 1`, the intersect is at `0`. This update has not only rotated the vector `W` to point towards `p1` but shifted the decision boundary downwards pulling the boundary away from `p1` so that `p1` is farther within the region of positivity and therefore more positive.\n",
    "\n",
    "The perceptron learning rule formula is as follows:\n",
    "\n",
    "$$ W_t := W_{t-1} + (target - activation) \\otimes input^{T} $$\n",
    "\n",
    "$$ b_t = b_{t-1} + (target - activation) $$\n",
    "\n",
    "`t` is the update iteration timestep. The circle with an x inside is the outer product, this fits with the shape of W since if the activation is m-dimensional, and input is n-dimensional, the W is shape mxn and the bias is shape mx1. Therefore the the update to W must be the same shape as W which is the shape you would get if you performed an outer product between (target - activation) x (input^T) = (update) which has shape (mx1)(1xn) = (mxn)\n",
    "\n",
    " In our example (target - activation) x (input^T) = (1 - 0) x `[[2],[1]]^T` = `[2,1]`\n",
    "\n",
    "For us the target vector is not some kind of supervised label, instead it is a target that the `backward feedback prediction functions (BFPF)` learns to come up with the target activations `z'` for all the layers of the `neural memory network` (NMN) in such a way as to be the target int he update rule to bind the vector to vector mapping of key value pairs.\n",
    "\n",
    "Our memory is stored in a multi-layer feed forward NMN\n",
    "\n",
    "$$ v^{r}_{t,i} = {NMN}(k^{r}_{t,i})$$\n",
    "\n",
    "$$ z'^{l} = {BFPF}^{l}(v^{w}_{}t) $$\n",
    "\n",
    "v^w is the memorywe want to remember, ie write to memory. k^r is the thought that occurs in real time that prompts us to retrieve v^r. \n",
    "\n",
    "v^r is the retreived memory for timestep t and i-th head, i-th because there might be several memories we want to retreive, t because we might be performing this retrieval for each step in a task.\n",
    "\n",
    "Lets re-write the above perceptron rule in a format that is analoguous to the layer by layer update rule in [Metalearned Neural Memory](https://arxiv.org/abs/1907.09720)\n",
    "\n",
    "$$ W_t := W_{t-1} - (activation - target) \\otimes x^{T} $$\n",
    "\n",
    "M^l is the weight matrix for layer l of the NMN, t is the new matrix after update, t-1 is the old matrix before update, B is a learning rate, z^l is the current activation for layer l prior to update, z'^l is the target activation for layer l. z^{l-1} are activations of the previous layer\n",
    "\n",
    "$$ M^{l}_{t} :=  M^{l}_{t-1} - B^{l}_{t}(z^{l} - z'^{l}) {z^{l-1}}^{T} $$\n",
    "\n",
    "These concepts are reflected in `class FFMemoryLearned(nn.Module):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFMemoryLearned(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    This is a minimal 3 layer feed forward neural net with\n",
    "    methods for meta-learned, aka, fast local updates\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, dim_hidden):\n",
    "        \n",
    "        super(FFMemoryLearned, self).__init__()\n",
    "        \n",
    "        # In the future you can use the get_clones function in TransformerComponents\n",
    "        # that uses the nn.ModuleList object to make several copies of the \n",
    "        self.l1 = nn.Linear(dim_hidden, dim_hidden).weight.data\n",
    "        self.l2 = nn.Linear(dim_hidden, dim_hidden).weight.data\n",
    "        self.l3 = nn.Linear(dim_hidden, dim_hidden).weight.data\n",
    "        \n",
    "        self.Ws = [self.l1, self.l2, self.l3]\n",
    "        self.Ws_temp = None\n",
    "        \n",
    "        # the 3 in  3*dim_hidden just reflects the fact that we define 3 layers above\n",
    "        self.expected_activation = nn.Linear(dim_hidden, 3*dim_hidden)\n",
    "\n",
    "    def if_cuda(self):\n",
    "        \n",
    "        if next(self.parameters()).is_cuda:\n",
    "            self.l1 = self.l1.cuda()\n",
    "            self.l2 = self.l2.cuda()\n",
    "            self.l3 = self.l3.cuda()\n",
    "            \n",
    "        self.Ws = [self.l1, self.l2, self.l3]\n",
    "\n",
    "    def detach_mem(self):\n",
    "        \n",
    "        ''' \n",
    "        In order to enable automatic differentiation, PyTorch keeps track of all operations involving\n",
    "        tensors for which the gradient may need to be computed (i.e., require_grad is True). \n",
    "        The operations are recorded as a directed graph. The detach() method constructs a new view on a \n",
    "        tensor which is declared not to need gradients, \n",
    "        i.e., it is to be excluded from further tracking of operations, and therefore \n",
    "        the sub-graph involving this view is not recorded.\n",
    "        .detach() doesnt implicitly create a copy of the tensor, so when the tensor is modified later, \n",
    "        itâ€™s updating the tensor on the upstream side of .detach()\n",
    "        '''\n",
    "        \n",
    "        for W_l in self.Ws:\n",
    "            \n",
    "            W_l.detach()\n",
    "            \n",
    "        self.Ws_temp = None\n",
    "\n",
    "    def forward(self, key):\n",
    "        \n",
    "        '''\n",
    "        The forward pass through the neural memory network (NMN).\n",
    "        This is used both in the read (retreive) and write (update) operations to memory\n",
    "        \n",
    "        Input:\n",
    "            key - type (torch.Tensor) shape (batch_size, num_heads, dim_hidden): \n",
    "                this is either the read_key or write_key input to the NMN\n",
    "        Output:\n",
    "            value - (torch.Tensor) shape (batch_size, num_heads, dim_hidden):\n",
    "                the output, aka last layer activation, from the NMN \n",
    "            activations - (list) of length (num layers - 1): \n",
    "                the layer activations, except the last one, in an ordered list\n",
    "                each of shape [batch_size, num_heads, dim_hidden])\n",
    "        '''\n",
    "        \n",
    "        self.if_cuda()\n",
    "        \n",
    "        # is reset at beginning of every new sequence \n",
    "        # If self.Ws_temp is not None, thats because we have just done a batched update\n",
    "        # and self.Ws_temp has a matrix for each batch sample \n",
    "        if self.Ws_temp is None: \n",
    "            \n",
    "          self.Ws_temp = []\n",
    "        \n",
    "          for W_l in self.Ws:\n",
    "                \n",
    "            # replicate weights of each layer across each batch sample\n",
    "            # (emb_dim, emb_dim) -> (batch_size, emb_dim, emb_dim)\n",
    "            \n",
    "            W = W_l.unsqueeze(dim=0).expand((key.shape[0], W_l.shape[0], W_l.shape[1]))\n",
    "\n",
    "            self.Ws_temp.append(W)\n",
    "            \n",
    "        activations = [] # hidden activations except the last  \n",
    "        \n",
    "        a = key\n",
    "        \n",
    "        for W_l in self.Ws_temp[:-1]:\n",
    "\n",
    "            a = torch.matmul(a, W_l.transpose(1,2))\n",
    "            a = torch.sigmoid(a)\n",
    "            activations.append(a)\n",
    "        \n",
    "        value = torch.matmul(a, self.Ws_temp[-1].transpose(1,2))\n",
    "\n",
    "        return value, activations\n",
    "\n",
    "    def mse_loss(self, y_pred, y):\n",
    "        'mean squared error'\n",
    "        \n",
    "        diff = y_pred - y\n",
    "        diff = diff.view(-1)\n",
    "        mse = diff.dot(diff)/diff.size()[0]\n",
    "        \n",
    "        return mse\n",
    "\n",
    "    def read(self, k, weights=None, avg=True):\n",
    "        'read from memory'\n",
    "        \n",
    "        v, h_acts = self.forward(k)\n",
    "\n",
    "        if weights is not None:\n",
    "          v *= weights\n",
    "        if avg:\n",
    "          v = v.mean(dim=1)\n",
    "        \n",
    "        return v\n",
    "\n",
    "    def update(self, k_w, v_w, beta_rate=0.1):  \n",
    "        \n",
    "        '''\n",
    "        The forward function is run twice, once before and once after the local update\n",
    "        k_w, v_w - shape [batch_size, num_heads, dim_hidden]\n",
    "        \n",
    "        '''\n",
    "\n",
    "        v_w_approx, activations = self.forward(k_w)\n",
    "        \n",
    "        v_w_approx = v_w_approx.contiguous()\n",
    "        v_w = v_w.contiguous()\n",
    "        \n",
    "        # before local update, see how well the this key and value are already bound\n",
    "        reconst_loss_init=self.mse_loss(v_w_approx.view(-1,v_w_approx.shape[2]),v_w.view(-1,v_w.shape[2]))\n",
    "        \n",
    "        # backward feedback prediction functions predict the expected activations of each NMN layer\n",
    "        # v_w.view(-1, v_w.shape[2]) performs shape change \n",
    "        # [batch_size, num_heads, emb_dim] -> [batch_size*num_heads, emb_dim]\n",
    "\n",
    "        # z_pr is the target activations for each layer\n",
    "        z_pr = self.expected_activation(v_w.view(-1, v_w.shape[2]))\n",
    "        # output z_pr is of shape [batch_size*num_heads, emb_dim*num_layers]\n",
    "        z_pr = z_pr.view(v_w.shape[0], v_w.shape[1], -1)\n",
    "        # output z_pr is of shape [batch_size, num_heads, emb_dim*num_layers]\n",
    "        z_pr = torch.chunk(z_pr, 3, dim=2) # result is a tuple of length 3\n",
    "        # each element in z_pr is shape [batch_size, num_heads, emb_dim]\n",
    "        \n",
    "        if len(beta_rate.shape) < 3:\n",
    "            beta_rate = beta_rate.unsqueeze(1)\n",
    "\n",
    "        # z2 and z1 are offset by 1\n",
    "        z2 = activations + [v_w_approx] # z2 are the initial output activations of the 3 layers\n",
    "        z1 = [k_w] + activations # z1 are the initial input activations of the 3 layers\n",
    "        \n",
    "        # Ws_t is a list of the updated weights of the NMN \n",
    "        Ws_t = []\n",
    "        \n",
    "        '''\n",
    "        # iterates through the 4 lists in reverse order as x_, applying the perceptron learning rule \n",
    "        # diff is shape (batchsize, heads, dim_hidden)\n",
    "        for W_l, z2_, z1_, z_pr in reversed(list(zip(self.Ws_temp, z2, z1, z_pr))):\n",
    "            \n",
    "            z1_ = z1_*beta_rate.expand(z1_.shape) # \n",
    "            diff = z2_ - z_pr # activation minus target_activation \n",
    "            diff = diff*(2./(diff.shape[1]*diff.shape[2])) # re-normalize diff so it doesnt explode with larger dims\n",
    "            W_l = W_l - torch.matmul(diff.transpose(1,2), z1_) #-0.0001*W_l 0.1\n",
    "            Ws_t.insert(0, W_l)\n",
    "        '''\n",
    "        \n",
    "        # iterates through the 4 lists in reverse order as x_, applying the perceptron learning rule \n",
    "        # diff is shape (batchsize, heads, dim_hidden)\n",
    "        for W_l, z2_, z1_, z_pr in list(zip(self.Ws_temp, z2, z1, z_pr)):\n",
    "            \n",
    "            \n",
    "            diff = z2_ - z_pr # activation minus target_activation, \n",
    "            diff = diff*beta_rate.expand(diff.shape) # diff.shape (batch_size, heads, emb_dim)\n",
    "            diff = diff/(diff.shape[1]*diff.shape[2]) #re-normalize prevents explosion with larger dims\n",
    "            # transpose(1,2) switches the emb_dim and heads dims so you get:\n",
    "            #(batch_size,emb_dim,heads)x(batch_size,heads,emb_dim)=(batch_size, emb_dim, emb_dim)\n",
    "            W_update = torch.matmul(diff.transpose(1,2), z1_)\n",
    "            \n",
    "            W_l = W_l - W_update \n",
    "            \n",
    "            Ws_t.append(W_l)\n",
    "        \n",
    "        self.Ws_temp[:] = Ws_t\n",
    "        \n",
    "        # Run memory forward again after memory update to see how well\n",
    "        # the v_w has been reconstructed as v_w_approx, use mse to\n",
    "        # quantify the quality of the key value binding as the reconst_loss\n",
    "        \n",
    "        v_w_approx, activations = self.forward(k_w)\n",
    "        \n",
    "        reconst_loss = self.mse_loss(v_w_approx.view(-1, v_w_approx.shape[2]), v_w.view(-1, v_w.shape[2]))\n",
    "        \n",
    "        return reconst_loss, reconst_loss_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product increases as the dimensions get larger. A fix is to introduce a scaling factor, and was proposed in Transformers [Attention Is All You Need](https://arxiv.org/abs/1706.03762). This function also has no learnable parameters. Above we scale down diff in proportion to the number of heads and number of hidden dimensions so that when we perform the matrix multiply with the previous activations, this dot product does not blow up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bot(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim, n_layers, heads, dropout, vocab):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.sos_tok = torch.LongTensor([[self.vocab.word2index[\"<SOS>\"]]]) \n",
    "        self.eos_tok = torch.LongTensor([[self.vocab.word2index[\"<EOS>\"]]]) \n",
    "        \n",
    "        self.encodeInput = Transformer(emb_dim, n_layers, heads, dropout)\n",
    "        self.encodeEncoding = Transformer(emb_dim, n_layers, heads, dropout)\n",
    "        self.decodeEncoding = Transformer(emb_dim, n_layers, heads, dropout)\n",
    "\n",
    "        self.mnm = MNMp(emb_dim, heads)\n",
    "        \n",
    "        self.context_vec = None\n",
    "        \n",
    "    def memory_utils(self, batch_size):\n",
    "\n",
    "        if self.context_vec is None:\n",
    "            cntxt_seq_len = 1\n",
    "            self.context_vec = torch.randn(batch_size, cntxt_seq_len, self.emb_dim)\n",
    "            \n",
    "        if self.context_vec.shape[0] > batch_size:\n",
    "            self.context_vec = self.context_vec[0,:,:]\n",
    "            \n",
    "        if self.context_vec.shape[0] < batch_size:\n",
    "            self.context_vec = self.context_vec[0,:,:].repeat(batch_size, 1, 1)\n",
    "    \n",
    "        self.context_vec = self.context_vec.detach()\n",
    "        self.mnm.memfunc.detach_mem()\n",
    "        \n",
    "    def forward(self, in_toks, in_mask, out_toks, out_mask):\n",
    "        \n",
    "        self.memory_utils(batch_size = in_toks.shape[0])\n",
    "        \n",
    "        in_vecs = self.vocab.embedding(in_toks)\n",
    "        out_vec = self.vocab.embedding(out_toks)\n",
    "\n",
    "        self.context_vec, rcl, rcli = self.mnm(self.context_vec)\n",
    "        encin_vec = self.encodeInput(in_vecs, in_mask, self.context_vec, None)\n",
    "        self.context_vec = self.encodeEncoding(self.context_vec, None, encin_vec, None)\n",
    "        \n",
    "        dout = self.decodeEncoding(out_vec, out_mask, encin_vec, in_mask)\n",
    "        \n",
    "        return dout, rcl, rcli\n",
    "    \n",
    "    def teacher_forcing(self, src, trg):\n",
    "        \n",
    "        self.train()\n",
    "        trg_start = torch.cat((self.sos_tok.repeat(trg.shape[0],1), trg),dim=1)\n",
    "        trg_end = torch.cat((trg, self.eos_tok.repeat(trg.shape[0],1)),dim=1)\n",
    "        src_mask = (src != self.vocab.word2index[\"<PAD>\"]).unsqueeze(-2)\n",
    "        trg_mask = (trg_end != self.vocab.word2index[\"<PAD>\"]).unsqueeze(-2)\n",
    "        \n",
    "        seq_len = trg_start.size(1) \n",
    "        np_mask = np.triu(np.ones((1,seq_len,seq_len)),k=1).astype('uint8')\n",
    "        np_mask =  torch.from_numpy(np_mask) == 0\n",
    "        \n",
    "        if trg.is_cuda:\n",
    "            np_mask = np_mask.cuda()\n",
    "            \n",
    "        trg_mask = trg_mask & np_mask\n",
    "        \n",
    "        out_vecs, rcl, rcli = self.forward(src, src_mask, trg_start, trg_mask)\n",
    "        \n",
    "        return out_vecs, trg_end, rcl, rcli\n",
    "    \n",
    "    def string2string(self, input_string, maxlen = 20):\n",
    "        \n",
    "        self.eval()\n",
    "        in_toks = self.vocab.string2tensor(input_string)\n",
    "        in_vecs = self.vocab.embedding(in_toks)\n",
    "        \n",
    "        self.memory_utils(batch_size=in_toks.shape[0])\n",
    "        \n",
    "        self.context_vec, rcl, rcli = self.mnm(self.context_vec)\n",
    "        encin_vec = self.encodeInput(in_vecs, None, self.context_vec, None)\n",
    "        self.context_vec = self.encodeEncoding(self.context_vec, None, encin_vec, None)\n",
    "        \n",
    "        decode_toks = self.sos_tok\n",
    "        \n",
    "        for pos in range(maxlen):\n",
    "            \n",
    "            decode_vecs = self.vocab.embedding(decode_toks)\n",
    "            dout = self.decodeEncoding(decode_vecs, None, encin_vec, None)\n",
    "            vocabdist = self.vocab.emb2vocab(dout)\n",
    "            next_toks = torch.argmax(vocabdist, dim=2)\n",
    "            decode_toks = torch.cat((decode_toks, next_toks[:,-1].unsqueeze(0)), dim=1) \n",
    "            \n",
    "            if next_toks[:,-1] == self.eos_tok.squeeze(0):\n",
    "                \n",
    "                toks = decode_toks[0][1:-1].data.cpu().numpy()\n",
    "                de_str = ' '.join([self.vocab.index2word[int(tok)] for tok in toks])\n",
    "\n",
    "                return de_str\n",
    "            \n",
    "        toks = decode_toks[0].data.cpu().numpy()\n",
    "        de_str = ' '.join([self.vocab.index2word[tok] for tok in toks])\n",
    "        \n",
    "        return de_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, 'my': 4, 'name': 5, 'is': 6, ',': 7, 'hi': 8, '.': 9, 'what': 10, '?': 11, 'its': 12}\n"
     ]
    }
   ],
   "source": [
    "emb_dim, n_layers, heads, dropout = 8, 3, 5, 0.01\n",
    "\n",
    "vocab = Vocab(emb_dim)\n",
    "model = Bot(emb_dim, n_layers, heads, dropout, vocab)\n",
    "teacher = Teacher(model.vocab)\n",
    "\n",
    "print(model.vocab.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best...\n",
      "mean accuracy 0.6667 celoss 1.0718 rcloss 3.7e-05 d_rcloss 0.0089 training progress 0.0 learning rate 0.001\n",
      "mean accuracy 0.6667 celoss 0.902 rcloss 2e-06 d_rcloss 0.0088 training progress 0.021 learning rate 0.001\n",
      "New Best...\n",
      "mean accuracy 0.7222 celoss 0.9539 rcloss 0.0 d_rcloss 0.0087 training progress 0.042 learning rate 0.001\n",
      "mean accuracy 0.7083 celoss 1.2834 rcloss 4e-06 d_rcloss 0.0088 training progress 0.063 learning rate 0.001\n",
      "mean accuracy 0.7 celoss 0.8586 rcloss 5e-06 d_rcloss 0.0087 training progress 0.084 learning rate 0.001\n",
      "mean accuracy 0.7222 celoss 0.8369 rcloss 2e-06 d_rcloss 0.0087 training progress 0.105 learning rate 0.001\n",
      "New Best...\n",
      "mean accuracy 0.7381 celoss 0.8917 rcloss 1e-06 d_rcloss 0.0087 training progress 0.126 learning rate 0.001\n",
      "mean accuracy 0.7292 celoss 0.8877 rcloss 4e-06 d_rcloss 0.0088 training progress 0.147 learning rate 0.001\n",
      "mean accuracy 0.7222 celoss 1.1306 rcloss 0.0 d_rcloss 0.0086 training progress 0.168 learning rate 0.001\n",
      "mean accuracy 0.7167 celoss 1.0071 rcloss 1e-06 d_rcloss 0.0086 training progress 0.189 learning rate 0.001\n",
      "mean accuracy 0.7333 celoss 1.0278 rcloss 4e-06 d_rcloss 0.0086 training progress 0.21 learning rate 0.001\n",
      "New Best...\n",
      "mean accuracy 0.75 celoss 1.0094 rcloss 2e-06 d_rcloss 0.0086 training progress 0.231 learning rate 0.001\n",
      "mean accuracy 0.7333 celoss 0.9478 rcloss 1e-06 d_rcloss 0.0086 training progress 0.252 learning rate 0.001\n"
     ]
    }
   ],
   "source": [
    "total_batches = 1000\n",
    "best_acc = 0\n",
    "lamda = 8\n",
    "batch_size = 2\n",
    "learning_rate = 0.001 \n",
    "\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate,betas=(0.9, 0.98),eps=1e-9)\n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min',factor=0.99,patience=100)\n",
    "\n",
    "loss_all_list = []\n",
    "rcloss_all_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "for batch in range(total_batches):\n",
    "    \n",
    "    #print('remember info')\n",
    "    \n",
    "    intro, introtarget, whatmyname, yournameis = teacher.get_batch(batch_size)\n",
    "    \n",
    "    out_vecs, trg_end, rcl, rcli = model.teacher_forcing(intro, introtarget)\n",
    "    \n",
    "    vocab_logits = model.vocab.emb2vocab(out_vecs)\n",
    "    \n",
    "    predictions = vocab_logits.view(-1, vocab_logits.size(-1))\n",
    "    \n",
    "    target = trg_end.view(-1)\n",
    "\n",
    "    batch_loss = F.cross_entropy(predictions, target, \n",
    "                                 ignore_index = model.vocab.word2index[\"<PAD>\"])\n",
    "\n",
    "    reconstruction_loss = lamda*rcl\n",
    "    \n",
    "    ################# Next Part of Conversation ########################\n",
    "    \n",
    "    #print('retreive memory')\n",
    "    \n",
    "    out_vecs, trg_end, rcl, rcli = model.teacher_forcing(whatmyname, yournameis)\n",
    "    \n",
    "    vocab_logits = model.vocab.emb2vocab(out_vecs)\n",
    "\n",
    "    predictions = vocab_logits.view(-1, vocab_logits.size(-1))\n",
    "    \n",
    "    target = trg_end.view(-1)\n",
    "    \n",
    "    acc = accuracy_score(target, torch.argmax(predictions, dim=1))\n",
    "\n",
    "    batch_loss += F.cross_entropy(predictions, target, \n",
    "                                 ignore_index = model.vocab.word2index[\"<PAD>\"])\n",
    "    \n",
    "    reconstruction_loss += lamda*rcl\n",
    "    conversation_loss = batch_loss + reconstruction_loss\n",
    "    \n",
    "    scheduler.step(conversation_loss)\n",
    "    optimizer.zero_grad()\n",
    "    conversation_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if batch % int(total_batches/50 + 1) == 0:\n",
    "        \n",
    "        loss_all_list.append(conversation_loss.float().item())\n",
    "        rcloss_all_list.append(reconstruction_loss.float().item())\n",
    "        accuracy_list.append(acc)\n",
    "        mean_accuracy = np.mean(accuracy_list[-10:])\n",
    "\n",
    "        if mean_accuracy > best_acc:\n",
    "            print('New Best...')\n",
    "            best_acc = mean_accuracy\n",
    "            \n",
    "        print(\n",
    "              \"mean accuracy\", round(mean_accuracy,4), \n",
    "              \"celoss\", round(batch_loss.float().item(),4), \n",
    "              \"rcloss\", round(reconstruction_loss.float().item(),6), \n",
    "              \"d_rcloss\", round((rcli - rcl).float().item(),4),\n",
    "              \"training progress\", round(batch/total_batches,4),\n",
    "              \"learning rate\", round(scheduler._last_lr[0],6),\n",
    "             )\n",
    "            \n",
    "        if mean_accuracy > 0.96:\n",
    "            break\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
    "fig.suptitle('Training Curves')\n",
    "ax1.set(xlabel='epochs', ylabel='train loss')\n",
    "ax2.set(xlabel='epochs', ylabel='reconstr loss')\n",
    "ax3.set(xlabel='epochs', ylabel='accuracy')\n",
    "ax1.plot(loss_all_list, label='train loss')\n",
    "ax2.plot(rcloss_all_list, label='reconstrunction loss')\n",
    "ax3.plot(accuracy_list, label='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 'three')\n",
      "(2, 'two')\n",
      "(1, 'one')\n"
     ]
    }
   ],
   "source": [
    "number_list = [1, 2, 3]\n",
    "str_list = ['one', 'two', 'three']\n",
    "\n",
    "# Two iterables are passed\n",
    "result = zip(number_list, str_list)\n",
    "\n",
    "# Converting iterator to set\n",
    "\n",
    "for tup in reversed(list(result)):\n",
    "    print(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
