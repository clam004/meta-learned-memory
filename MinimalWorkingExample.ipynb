{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.version 1.7.1\n",
      "torch.cuda.is_available() False\n",
      "torch.cuda.device_count() 0\n"
     ]
    }
   ],
   "source": [
    "import math, copy, sys, logging, json, time, random, os, string, pickle, re\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modules.TransformerComponents import Transformer\n",
    "from modules.Vocabulary import Vocab\n",
    "from modules.MetaLearnNeuralMemory import MNMp\n",
    "from modules.LoadTrainSave import save_model, load_model\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(0) \n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print('torch.version', torch.__version__)\n",
    "print('torch.cuda.is_available()', torch.cuda.is_available())\n",
    "print('torch.cuda.device_count()', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Working Example\n",
    "\n",
    "This notebook serves as a minimal working example of Meta-Learned Nerual Memory Chatbot (MNMC). The Teacher teaches a simple conversational lesson, \"when I tell you a name, remember my name, when I ask you want my name is, tell me what my name is, if I tell you I have a different name, memorize that new name, when I ask you what my name is, tell me the most recent name I told you\" \n",
    "\n",
    "# Why is this special?\n",
    "\n",
    "Seems like a somewhat mundane task, however, the mechanism by which it is accomplished is what is profound. The \"name\" is not stored in any kind of traditional database or data structure, but rather has been stored in the distributed, aka, shared, weights of a neural network. No new row or element is added to a table or array when the name is remembered, instead the values of a finite set of weights is adjusted. \n",
    "\n",
    "# Explaination\n",
    "\n",
    "Here we show an end to end, albeit minimal, example, the subsequent notebooks will explain Metalearned Nerual memory and how we have combined this with the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher(): \n",
    "    \n",
    "    def __init__(self, vocab):\n",
    "        \n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.vocab.string2embedding(\"my name is, hi. what is my name? its\")\n",
    "        \n",
    "        self.name_list = [\n",
    "                          'vicki', 'carson', 'melissa', 'salvador', \n",
    "                          'force', 'sky', 'zen', 'adam'\n",
    "                         ]\n",
    "\n",
    "    def random_name(self,):\n",
    "        \"\"\" Generate a random string of fixed length \"\"\"\n",
    "        return random.choice(self.name_list)\n",
    "    \n",
    "    def repeat(self, batch_size):\n",
    "        \n",
    "        self.mynameis = self.vocab.string2tensor(\"my name is\")\n",
    "        self.hi = self.vocab.string2tensor(\"hi\")\n",
    "        self.whatmyname = self.vocab.string2tensor(\"what is my name?\")\n",
    "        self.its = self.vocab.string2tensor(\"its\")\n",
    "\n",
    "        self.mynameis = self.mynameis.repeat(batch_size,1)\n",
    "        self.hi = self.hi.repeat(batch_size,1)\n",
    "        self.whatmyname = self.whatmyname.repeat(batch_size,1)\n",
    "        self.its = self.its.repeat(batch_size,1)\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        \n",
    "        self.repeat(batch_size)\n",
    "        \n",
    "        newnames = \"\"\n",
    "        for n in range(batch_size):\n",
    "            newnames += \" \" + self.random_name()\n",
    "            \n",
    "        self.vocab.string2embedding(newnames)\n",
    "        \n",
    "        self.names = self.vocab.string2tensor(newnames).T\n",
    "\n",
    "        self.intro = torch.cat((self.mynameis, self.names),dim=1)\n",
    "        self.introtarget = torch.cat((self.hi, self.names),dim=1)\n",
    "        self.yournameis = torch.cat((self.its, self.names),dim=1)\n",
    "        \n",
    "        return self.intro, self.introtarget, self.whatmyname, self.yournameis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration of the training data outputted from the teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, 'my': 4, 'name': 5, 'is': 6, ',': 7, 'hi': 8, '.': 9, 'what': 10, '?': 11, 'its': 12}\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(emb_dim=32)\n",
    "teacher = Teacher(vocab)\n",
    "\n",
    "print(vocab.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, 'my': 4, 'name': 5, 'is': 6, ',': 7, 'hi': 8, '.': 9, 'what': 10, '?': 11, 'its': 12, 'zen': 13, 'vicki': 14, 'force': 15}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "intro, introtarget, whatmyname, yournameis = teacher.get_batch(batch_size)\n",
    "\n",
    "print(vocab.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  5,  6, 13],\n",
       "        [ 4,  5,  6, 13],\n",
       "        [ 4,  5,  6, 14],\n",
       "        [ 4,  5,  6, 15]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro # my name is <new token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8, 13],\n",
       "        [ 8, 13],\n",
       "        [ 8, 14],\n",
       "        [ 8, 15]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "introtarget # hi <new token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10,  6,  4,  5, 11],\n",
       "        [10,  6,  4,  5, 11],\n",
       "        [10,  6,  4,  5, 11],\n",
       "        [10,  6,  4,  5, 11]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whatmyname # what is my name ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12, 13],\n",
       "        [12, 13],\n",
       "        [12, 14],\n",
       "        [12, 15]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yournameis # its <new token>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Chatbot (Bot)\n",
    "\n",
    "With built in memory unit, training unit (teacher forcing) and chatting unit (string2string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bot(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim, n_layers, heads, dropout, vocab):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.sos_tok = torch.LongTensor([[self.vocab.word2index[\"<SOS>\"]]]) \n",
    "        self.eos_tok = torch.LongTensor([[self.vocab.word2index[\"<EOS>\"]]]) \n",
    "        \n",
    "        self.encodeInput = Transformer(emb_dim, n_layers, heads, dropout)\n",
    "        self.encodeEncoding = Transformer(emb_dim, n_layers, heads, dropout)\n",
    "        self.decodeEncoding = Transformer(emb_dim, n_layers, heads, dropout)\n",
    "\n",
    "        self.mnm = MNMp(emb_dim, heads)\n",
    "        \n",
    "        self.context_vec = None\n",
    "        \n",
    "    def memory_utils(self, batch_size):\n",
    "\n",
    "        if self.context_vec is None:\n",
    "            cntxt_seq_len = 1\n",
    "            self.context_vec = torch.randn(batch_size, cntxt_seq_len, self.emb_dim)\n",
    "            \n",
    "        if self.context_vec.shape[0] > batch_size:\n",
    "            self.context_vec = self.context_vec[0,:,:]\n",
    "            \n",
    "        if self.context_vec.shape[0] < batch_size:\n",
    "            self.context_vec = self.context_vec[0,:,:].repeat(batch_size, 1, 1)\n",
    "    \n",
    "        self.context_vec = self.context_vec.detach()\n",
    "        self.mnm.memfunc.detach_mem()\n",
    "        \n",
    "    def forward(self, in_toks, in_mask, out_toks, out_mask):\n",
    "        \n",
    "        self.memory_utils(batch_size = in_toks.shape[0])\n",
    "        \n",
    "        in_vecs = self.vocab.embedding(in_toks)\n",
    "        out_vec = self.vocab.embedding(out_toks)\n",
    "\n",
    "        self.context_vec, rcl, rcli = self.mnm(self.context_vec)\n",
    "        encin_vec = self.encodeInput(in_vecs, in_mask, self.context_vec, None)\n",
    "        self.context_vec = self.encodeEncoding(self.context_vec, None, encin_vec, None)\n",
    "        \n",
    "        dout = self.decodeEncoding(out_vec, out_mask, encin_vec, in_mask)\n",
    "        \n",
    "        return dout, rcl, rcli\n",
    "    \n",
    "    def teacher_forcing(self, src, trg):\n",
    "        \n",
    "        self.train()\n",
    "        trg_start = torch.cat((self.sos_tok.repeat(trg.shape[0],1), trg),dim=1)\n",
    "        trg_end = torch.cat((trg, self.eos_tok.repeat(trg.shape[0],1)),dim=1)\n",
    "        src_mask = (src != self.vocab.word2index[\"<PAD>\"]).unsqueeze(-2)\n",
    "        trg_mask = (trg_end != self.vocab.word2index[\"<PAD>\"]).unsqueeze(-2)\n",
    "        \n",
    "        seq_len = trg_start.size(1) \n",
    "        np_mask = np.triu(np.ones((1,seq_len,seq_len)),k=1).astype('uint8')\n",
    "        np_mask =  torch.from_numpy(np_mask) == 0\n",
    "        \n",
    "        if trg.is_cuda:\n",
    "            np_mask = np_mask.cuda()\n",
    "            \n",
    "        trg_mask = trg_mask & np_mask\n",
    "        \n",
    "        out_vecs, rcl, rcli = self.forward(src, src_mask, trg_start, trg_mask)\n",
    "        \n",
    "        return out_vecs, trg_end, rcl, rcli\n",
    "    \n",
    "    def string2string(self, input_string, maxlen = 20):\n",
    "        \n",
    "        self.eval()\n",
    "        in_toks = self.vocab.string2tensor(input_string)\n",
    "        in_vecs = self.vocab.embedding(in_toks)\n",
    "        \n",
    "        self.memory_utils(batch_size=in_toks.shape[0])\n",
    "        \n",
    "        self.context_vec, rcl, rcli = self.mnm(self.context_vec)\n",
    "        encin_vec = self.encodeInput(in_vecs, None, self.context_vec, None)\n",
    "        self.context_vec = self.encodeEncoding(self.context_vec, None, encin_vec, None)\n",
    "        \n",
    "        decode_toks = self.sos_tok\n",
    "        \n",
    "        for pos in range(maxlen):\n",
    "            \n",
    "            decode_vecs = self.vocab.embedding(decode_toks)\n",
    "            dout = self.decodeEncoding(decode_vecs, None, encin_vec, None)\n",
    "            vocabdist = self.vocab.emb2vocab(dout)\n",
    "            next_toks = torch.argmax(vocabdist, dim=2)\n",
    "            decode_toks = torch.cat((decode_toks, next_toks[:,-1].unsqueeze(0)), dim=1) \n",
    "            \n",
    "            if next_toks[:,-1] == self.eos_tok.squeeze(0):\n",
    "                \n",
    "                toks = decode_toks[0][1:-1].data.cpu().numpy()\n",
    "                de_str = ' '.join([self.vocab.index2word[int(tok)] for tok in toks])\n",
    "\n",
    "                return de_str\n",
    "            \n",
    "        toks = decode_toks[0].data.cpu().numpy()\n",
    "        de_str = ' '.join([self.vocab.index2word[tok] for tok in toks])\n",
    "        \n",
    "        return de_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the vocab, model and teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, 'my': 4, 'name': 5, 'is': 6, ',': 7, 'hi': 8, '.': 9, 'what': 10, '?': 11, 'its': 12}\n"
     ]
    }
   ],
   "source": [
    "emb_dim, n_layers, heads, dropout = 32, 2, 2, 0.05\n",
    "\n",
    "vocab = Vocab(emb_dim)\n",
    "model = Bot(emb_dim, n_layers, heads, dropout, vocab)\n",
    "teacher = Teacher(model.vocab)\n",
    "\n",
    "print(model.vocab.word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you dont get to > 90% accuracy the first time you can re-run the training cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model...\n",
      "mean accuracy 0.9062 celoss 1.91 rcloss 0.000536 d_rcloss 0.0991 training progress 0.0 learning rate [0.01]\n",
      "Saving Model...\n",
      "mean accuracy 0.9375 celoss 0.1086 rcloss 0.000593 d_rcloss 0.1049 training progress 0.0506 learning rate [0.01]\n",
      "Saving Model...\n",
      "mean accuracy 0.9479 celoss 0.1299 rcloss 0.00035 d_rcloss 0.1087 training progress 0.1013 learning rate [0.01]\n",
      "Saving Model...\n",
      "mean accuracy 0.9531 celoss 0.1107 rcloss 0.000418 d_rcloss 0.0946 training progress 0.1519 learning rate [0.0099]\n",
      "Saving Model...\n",
      "mean accuracy 0.9594 celoss 0.0757 rcloss 0.000552 d_rcloss 0.096 training progress 0.2025 learning rate [0.0099]\n",
      "Saving Model...\n",
      "mean accuracy 0.9618 celoss 0.1175 rcloss 0.000505 d_rcloss 0.0973 training progress 0.2531 learning rate [0.0099]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1763580d0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_batches = 64*25\n",
    "best_acc = 0\n",
    "lamda = 8\n",
    "batch_size = 64\n",
    "learning_rate = 0.01 \n",
    "\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate,betas=(0.9, 0.98),eps=1e-9)\n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min',factor=0.99,patience=100)\n",
    "\n",
    "loss_all_list = []\n",
    "rcloss_all_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "for batch in range(total_batches):\n",
    "    \n",
    "    intro, introtarget, whatmyname, yournameis = teacher.get_batch(batch_size)\n",
    "    \n",
    "    out_vecs, trg_end, rcl, rcli = model.teacher_forcing(intro, introtarget)\n",
    "    \n",
    "    vocab_logits = model.vocab.emb2vocab(out_vecs)\n",
    "    \n",
    "    predictions = vocab_logits.view(-1, vocab_logits.size(-1))\n",
    "    \n",
    "    target = trg_end.view(-1)\n",
    "\n",
    "    batch_loss = F.cross_entropy(predictions, target, \n",
    "                                 ignore_index = model.vocab.word2index[\"<PAD>\"])\n",
    "\n",
    "    reconstruction_loss = lamda*rcl\n",
    "    \n",
    "    ################# Next Part of Conversation ########################\n",
    "    \n",
    "    out_vecs, trg_end, rcl, rcli = model.teacher_forcing(whatmyname, yournameis)\n",
    "    \n",
    "    vocab_logits = model.vocab.emb2vocab(out_vecs)\n",
    "\n",
    "    predictions = vocab_logits.view(-1, vocab_logits.size(-1))\n",
    "    \n",
    "    target = trg_end.view(-1)\n",
    "    \n",
    "    acc = accuracy_score(target, torch.argmax(predictions, dim=1))\n",
    "\n",
    "    batch_loss += F.cross_entropy(predictions, target, \n",
    "                                 ignore_index = model.vocab.word2index[\"<PAD>\"])\n",
    "    \n",
    "    reconstruction_loss += lamda*rcl\n",
    "    conversation_loss = batch_loss + reconstruction_loss\n",
    "    \n",
    "    scheduler.step(conversation_loss)\n",
    "    optimizer.zero_grad()\n",
    "    conversation_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if batch % int(total_batches/20 + 1) == 0:\n",
    "        \n",
    "        loss_all_list.append(conversation_loss.float().item())\n",
    "        rcloss_all_list.append(reconstruction_loss.float().item())\n",
    "        accuracy_list.append(acc)\n",
    "        mean_accuracy = np.mean(accuracy_list[-10:])\n",
    "\n",
    "        if mean_accuracy > best_acc:\n",
    "            print('Saving Model...')\n",
    "            best_acc = mean_accuracy\n",
    "            \n",
    "            pickle.dump(model.vocab.word2index,open(\"modelstate/word2index.p\",\"wb\"))\n",
    "            pickle.dump(model.vocab.index2word,open(\"modelstate/index2word.p\",\"wb\"))\n",
    "            pickle.dump(model.vocab.emb2vocab.weight,open(\"modelstate/emb2vocab.weight.p\",\"wb\"))\n",
    "            pickle.dump(model.vocab.embedding.weight,open(\"modelstate/embedding.weight.p\",\"wb\"))\n",
    "            pickle.dump(model.context_vec,open(\"modelstate/context_vec.p\",\"wb\"))\n",
    "            pickle.dump(model.mnm.memfunc.Ws,open(\"modelstate/Ws.p\",\"wb\"))\n",
    "            save_model(model,\"modelstate/task.pth\")\n",
    "            \n",
    "        print(\"mean accuracy\", round(mean_accuracy,4), \n",
    "              \"celoss\", round(batch_loss.float().item(),4), \n",
    "              \"rcloss\", round(reconstruction_loss.float().item(),6), \n",
    "              \"d_rcloss\", round((rcli - rcl).float().item(),4),\n",
    "              \"training progress\", round(batch/total_batches,4),\n",
    "              \"learning rate\", scheduler._last_lr)\n",
    "            \n",
    "        if mean_accuracy > 0.96:\n",
    "            break\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
    "fig.suptitle('Training Curves')\n",
    "ax1.set(xlabel='epochs', ylabel='train loss')\n",
    "ax2.set(xlabel='epochs', ylabel='reconstr loss')\n",
    "ax3.set(xlabel='epochs', ylabel='accuracy')\n",
    "ax1.plot(loss_all_list, label='train loss')\n",
    "ax2.plot(rcloss_all_list, label='reconstrunction loss')\n",
    "ax3.plot(accuracy_list, label='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the names of all the trainable parameters, aka, weights in this model. there are a few other parameters that are not in the computational graph of pytorch, which is why there are other saved files to be saved and re-loaded besides `save_model(model,\"modelstate/task.pth\")` such as the embeddings, vocabulary dictionary, the vector that stores the last memory and the matrices that store the memories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab.embedding.weight\n",
      "vocab.emb2vocab.weight\n",
      "encodeInput.encodelayers.0.norm_1.alpha\n",
      "encodeInput.encodelayers.0.norm_1.bias\n",
      "encodeInput.encodelayers.0.attn.q_linear.weight\n",
      "encodeInput.encodelayers.0.attn.q_linear.bias\n",
      "encodeInput.encodelayers.0.attn.k_linear.weight\n",
      "encodeInput.encodelayers.0.attn.k_linear.bias\n",
      "encodeInput.encodelayers.0.attn.v_linear.weight\n",
      "encodeInput.encodelayers.0.attn.v_linear.bias\n",
      "encodeInput.encodelayers.0.attn.out.weight\n",
      "encodeInput.encodelayers.0.attn.out.bias\n",
      "encodeInput.encodelayers.0.norm_2.alpha\n",
      "encodeInput.encodelayers.0.norm_2.bias\n",
      "encodeInput.encodelayers.0.ff.linear_1.weight\n",
      "encodeInput.encodelayers.0.ff.linear_1.bias\n",
      "encodeInput.encodelayers.0.ff.linear_2.weight\n",
      "encodeInput.encodelayers.0.ff.linear_2.bias\n",
      "encodeInput.encodelayers.1.norm_1.alpha\n",
      "encodeInput.encodelayers.1.norm_1.bias\n",
      "encodeInput.encodelayers.1.attn.q_linear.weight\n",
      "encodeInput.encodelayers.1.attn.q_linear.bias\n",
      "encodeInput.encodelayers.1.attn.k_linear.weight\n",
      "encodeInput.encodelayers.1.attn.k_linear.bias\n",
      "encodeInput.encodelayers.1.attn.v_linear.weight\n",
      "encodeInput.encodelayers.1.attn.v_linear.bias\n",
      "encodeInput.encodelayers.1.attn.out.weight\n",
      "encodeInput.encodelayers.1.attn.out.bias\n",
      "encodeInput.encodelayers.1.norm_2.alpha\n",
      "encodeInput.encodelayers.1.norm_2.bias\n",
      "encodeInput.encodelayers.1.ff.linear_1.weight\n",
      "encodeInput.encodelayers.1.ff.linear_1.bias\n",
      "encodeInput.encodelayers.1.ff.linear_2.weight\n",
      "encodeInput.encodelayers.1.ff.linear_2.bias\n",
      "encodeInput.decodelayers.0.norm_1.alpha\n",
      "encodeInput.decodelayers.0.norm_1.bias\n",
      "encodeInput.decodelayers.0.norm_2.alpha\n",
      "encodeInput.decodelayers.0.norm_2.bias\n",
      "encodeInput.decodelayers.0.norm_3.alpha\n",
      "encodeInput.decodelayers.0.norm_3.bias\n",
      "encodeInput.decodelayers.0.attn_1.q_linear.weight\n",
      "encodeInput.decodelayers.0.attn_1.q_linear.bias\n",
      "encodeInput.decodelayers.0.attn_1.k_linear.weight\n",
      "encodeInput.decodelayers.0.attn_1.k_linear.bias\n",
      "encodeInput.decodelayers.0.attn_1.v_linear.weight\n",
      "encodeInput.decodelayers.0.attn_1.v_linear.bias\n",
      "encodeInput.decodelayers.0.attn_1.out.weight\n",
      "encodeInput.decodelayers.0.attn_1.out.bias\n",
      "encodeInput.decodelayers.0.attn_2.q_linear.weight\n",
      "encodeInput.decodelayers.0.attn_2.q_linear.bias\n",
      "encodeInput.decodelayers.0.attn_2.k_linear.weight\n",
      "encodeInput.decodelayers.0.attn_2.k_linear.bias\n",
      "encodeInput.decodelayers.0.attn_2.v_linear.weight\n",
      "encodeInput.decodelayers.0.attn_2.v_linear.bias\n",
      "encodeInput.decodelayers.0.attn_2.out.weight\n",
      "encodeInput.decodelayers.0.attn_2.out.bias\n",
      "encodeInput.decodelayers.0.ff.linear_1.weight\n",
      "encodeInput.decodelayers.0.ff.linear_1.bias\n",
      "encodeInput.decodelayers.0.ff.linear_2.weight\n",
      "encodeInput.decodelayers.0.ff.linear_2.bias\n",
      "encodeInput.decodelayers.1.norm_1.alpha\n",
      "encodeInput.decodelayers.1.norm_1.bias\n",
      "encodeInput.decodelayers.1.norm_2.alpha\n",
      "encodeInput.decodelayers.1.norm_2.bias\n",
      "encodeInput.decodelayers.1.norm_3.alpha\n",
      "encodeInput.decodelayers.1.norm_3.bias\n",
      "encodeInput.decodelayers.1.attn_1.q_linear.weight\n",
      "encodeInput.decodelayers.1.attn_1.q_linear.bias\n",
      "encodeInput.decodelayers.1.attn_1.k_linear.weight\n",
      "encodeInput.decodelayers.1.attn_1.k_linear.bias\n",
      "encodeInput.decodelayers.1.attn_1.v_linear.weight\n",
      "encodeInput.decodelayers.1.attn_1.v_linear.bias\n",
      "encodeInput.decodelayers.1.attn_1.out.weight\n",
      "encodeInput.decodelayers.1.attn_1.out.bias\n",
      "encodeInput.decodelayers.1.attn_2.q_linear.weight\n",
      "encodeInput.decodelayers.1.attn_2.q_linear.bias\n",
      "encodeInput.decodelayers.1.attn_2.k_linear.weight\n",
      "encodeInput.decodelayers.1.attn_2.k_linear.bias\n",
      "encodeInput.decodelayers.1.attn_2.v_linear.weight\n",
      "encodeInput.decodelayers.1.attn_2.v_linear.bias\n",
      "encodeInput.decodelayers.1.attn_2.out.weight\n",
      "encodeInput.decodelayers.1.attn_2.out.bias\n",
      "encodeInput.decodelayers.1.ff.linear_1.weight\n",
      "encodeInput.decodelayers.1.ff.linear_1.bias\n",
      "encodeInput.decodelayers.1.ff.linear_2.weight\n",
      "encodeInput.decodelayers.1.ff.linear_2.bias\n",
      "encodeInput.norm.alpha\n",
      "encodeInput.norm.bias\n",
      "encodeEncoding.encodelayers.0.norm_1.alpha\n",
      "encodeEncoding.encodelayers.0.norm_1.bias\n",
      "encodeEncoding.encodelayers.0.attn.q_linear.weight\n",
      "encodeEncoding.encodelayers.0.attn.q_linear.bias\n",
      "encodeEncoding.encodelayers.0.attn.k_linear.weight\n",
      "encodeEncoding.encodelayers.0.attn.k_linear.bias\n",
      "encodeEncoding.encodelayers.0.attn.v_linear.weight\n",
      "encodeEncoding.encodelayers.0.attn.v_linear.bias\n",
      "encodeEncoding.encodelayers.0.attn.out.weight\n",
      "encodeEncoding.encodelayers.0.attn.out.bias\n",
      "encodeEncoding.encodelayers.0.norm_2.alpha\n",
      "encodeEncoding.encodelayers.0.norm_2.bias\n",
      "encodeEncoding.encodelayers.0.ff.linear_1.weight\n",
      "encodeEncoding.encodelayers.0.ff.linear_1.bias\n",
      "encodeEncoding.encodelayers.0.ff.linear_2.weight\n",
      "encodeEncoding.encodelayers.0.ff.linear_2.bias\n",
      "encodeEncoding.encodelayers.1.norm_1.alpha\n",
      "encodeEncoding.encodelayers.1.norm_1.bias\n",
      "encodeEncoding.encodelayers.1.attn.q_linear.weight\n",
      "encodeEncoding.encodelayers.1.attn.q_linear.bias\n",
      "encodeEncoding.encodelayers.1.attn.k_linear.weight\n",
      "encodeEncoding.encodelayers.1.attn.k_linear.bias\n",
      "encodeEncoding.encodelayers.1.attn.v_linear.weight\n",
      "encodeEncoding.encodelayers.1.attn.v_linear.bias\n",
      "encodeEncoding.encodelayers.1.attn.out.weight\n",
      "encodeEncoding.encodelayers.1.attn.out.bias\n",
      "encodeEncoding.encodelayers.1.norm_2.alpha\n",
      "encodeEncoding.encodelayers.1.norm_2.bias\n",
      "encodeEncoding.encodelayers.1.ff.linear_1.weight\n",
      "encodeEncoding.encodelayers.1.ff.linear_1.bias\n",
      "encodeEncoding.encodelayers.1.ff.linear_2.weight\n",
      "encodeEncoding.encodelayers.1.ff.linear_2.bias\n",
      "encodeEncoding.decodelayers.0.norm_1.alpha\n",
      "encodeEncoding.decodelayers.0.norm_1.bias\n",
      "encodeEncoding.decodelayers.0.norm_2.alpha\n",
      "encodeEncoding.decodelayers.0.norm_2.bias\n",
      "encodeEncoding.decodelayers.0.norm_3.alpha\n",
      "encodeEncoding.decodelayers.0.norm_3.bias\n",
      "encodeEncoding.decodelayers.0.attn_1.q_linear.weight\n",
      "encodeEncoding.decodelayers.0.attn_1.q_linear.bias\n",
      "encodeEncoding.decodelayers.0.attn_1.k_linear.weight\n",
      "encodeEncoding.decodelayers.0.attn_1.k_linear.bias\n",
      "encodeEncoding.decodelayers.0.attn_1.v_linear.weight\n",
      "encodeEncoding.decodelayers.0.attn_1.v_linear.bias\n",
      "encodeEncoding.decodelayers.0.attn_1.out.weight\n",
      "encodeEncoding.decodelayers.0.attn_1.out.bias\n",
      "encodeEncoding.decodelayers.0.attn_2.q_linear.weight\n",
      "encodeEncoding.decodelayers.0.attn_2.q_linear.bias\n",
      "encodeEncoding.decodelayers.0.attn_2.k_linear.weight\n",
      "encodeEncoding.decodelayers.0.attn_2.k_linear.bias\n",
      "encodeEncoding.decodelayers.0.attn_2.v_linear.weight\n",
      "encodeEncoding.decodelayers.0.attn_2.v_linear.bias\n",
      "encodeEncoding.decodelayers.0.attn_2.out.weight\n",
      "encodeEncoding.decodelayers.0.attn_2.out.bias\n",
      "encodeEncoding.decodelayers.0.ff.linear_1.weight\n",
      "encodeEncoding.decodelayers.0.ff.linear_1.bias\n",
      "encodeEncoding.decodelayers.0.ff.linear_2.weight\n",
      "encodeEncoding.decodelayers.0.ff.linear_2.bias\n",
      "encodeEncoding.decodelayers.1.norm_1.alpha\n",
      "encodeEncoding.decodelayers.1.norm_1.bias\n",
      "encodeEncoding.decodelayers.1.norm_2.alpha\n",
      "encodeEncoding.decodelayers.1.norm_2.bias\n",
      "encodeEncoding.decodelayers.1.norm_3.alpha\n",
      "encodeEncoding.decodelayers.1.norm_3.bias\n",
      "encodeEncoding.decodelayers.1.attn_1.q_linear.weight\n",
      "encodeEncoding.decodelayers.1.attn_1.q_linear.bias\n",
      "encodeEncoding.decodelayers.1.attn_1.k_linear.weight\n",
      "encodeEncoding.decodelayers.1.attn_1.k_linear.bias\n",
      "encodeEncoding.decodelayers.1.attn_1.v_linear.weight\n",
      "encodeEncoding.decodelayers.1.attn_1.v_linear.bias\n",
      "encodeEncoding.decodelayers.1.attn_1.out.weight\n",
      "encodeEncoding.decodelayers.1.attn_1.out.bias\n",
      "encodeEncoding.decodelayers.1.attn_2.q_linear.weight\n",
      "encodeEncoding.decodelayers.1.attn_2.q_linear.bias\n",
      "encodeEncoding.decodelayers.1.attn_2.k_linear.weight\n",
      "encodeEncoding.decodelayers.1.attn_2.k_linear.bias\n",
      "encodeEncoding.decodelayers.1.attn_2.v_linear.weight\n",
      "encodeEncoding.decodelayers.1.attn_2.v_linear.bias\n",
      "encodeEncoding.decodelayers.1.attn_2.out.weight\n",
      "encodeEncoding.decodelayers.1.attn_2.out.bias\n",
      "encodeEncoding.decodelayers.1.ff.linear_1.weight\n",
      "encodeEncoding.decodelayers.1.ff.linear_1.bias\n",
      "encodeEncoding.decodelayers.1.ff.linear_2.weight\n",
      "encodeEncoding.decodelayers.1.ff.linear_2.bias\n",
      "encodeEncoding.norm.alpha\n",
      "encodeEncoding.norm.bias\n",
      "decodeEncoding.encodelayers.0.norm_1.alpha\n",
      "decodeEncoding.encodelayers.0.norm_1.bias\n",
      "decodeEncoding.encodelayers.0.attn.q_linear.weight\n",
      "decodeEncoding.encodelayers.0.attn.q_linear.bias\n",
      "decodeEncoding.encodelayers.0.attn.k_linear.weight\n",
      "decodeEncoding.encodelayers.0.attn.k_linear.bias\n",
      "decodeEncoding.encodelayers.0.attn.v_linear.weight\n",
      "decodeEncoding.encodelayers.0.attn.v_linear.bias\n",
      "decodeEncoding.encodelayers.0.attn.out.weight\n",
      "decodeEncoding.encodelayers.0.attn.out.bias\n",
      "decodeEncoding.encodelayers.0.norm_2.alpha\n",
      "decodeEncoding.encodelayers.0.norm_2.bias\n",
      "decodeEncoding.encodelayers.0.ff.linear_1.weight\n",
      "decodeEncoding.encodelayers.0.ff.linear_1.bias\n",
      "decodeEncoding.encodelayers.0.ff.linear_2.weight\n",
      "decodeEncoding.encodelayers.0.ff.linear_2.bias\n",
      "decodeEncoding.encodelayers.1.norm_1.alpha\n",
      "decodeEncoding.encodelayers.1.norm_1.bias\n",
      "decodeEncoding.encodelayers.1.attn.q_linear.weight\n",
      "decodeEncoding.encodelayers.1.attn.q_linear.bias\n",
      "decodeEncoding.encodelayers.1.attn.k_linear.weight\n",
      "decodeEncoding.encodelayers.1.attn.k_linear.bias\n",
      "decodeEncoding.encodelayers.1.attn.v_linear.weight\n",
      "decodeEncoding.encodelayers.1.attn.v_linear.bias\n",
      "decodeEncoding.encodelayers.1.attn.out.weight\n",
      "decodeEncoding.encodelayers.1.attn.out.bias\n",
      "decodeEncoding.encodelayers.1.norm_2.alpha\n",
      "decodeEncoding.encodelayers.1.norm_2.bias\n",
      "decodeEncoding.encodelayers.1.ff.linear_1.weight\n",
      "decodeEncoding.encodelayers.1.ff.linear_1.bias\n",
      "decodeEncoding.encodelayers.1.ff.linear_2.weight\n",
      "decodeEncoding.encodelayers.1.ff.linear_2.bias\n",
      "decodeEncoding.decodelayers.0.norm_1.alpha\n",
      "decodeEncoding.decodelayers.0.norm_1.bias\n",
      "decodeEncoding.decodelayers.0.norm_2.alpha\n",
      "decodeEncoding.decodelayers.0.norm_2.bias\n",
      "decodeEncoding.decodelayers.0.norm_3.alpha\n",
      "decodeEncoding.decodelayers.0.norm_3.bias\n",
      "decodeEncoding.decodelayers.0.attn_1.q_linear.weight\n",
      "decodeEncoding.decodelayers.0.attn_1.q_linear.bias\n",
      "decodeEncoding.decodelayers.0.attn_1.k_linear.weight\n",
      "decodeEncoding.decodelayers.0.attn_1.k_linear.bias\n",
      "decodeEncoding.decodelayers.0.attn_1.v_linear.weight\n",
      "decodeEncoding.decodelayers.0.attn_1.v_linear.bias\n",
      "decodeEncoding.decodelayers.0.attn_1.out.weight\n",
      "decodeEncoding.decodelayers.0.attn_1.out.bias\n",
      "decodeEncoding.decodelayers.0.attn_2.q_linear.weight\n",
      "decodeEncoding.decodelayers.0.attn_2.q_linear.bias\n",
      "decodeEncoding.decodelayers.0.attn_2.k_linear.weight\n",
      "decodeEncoding.decodelayers.0.attn_2.k_linear.bias\n",
      "decodeEncoding.decodelayers.0.attn_2.v_linear.weight\n",
      "decodeEncoding.decodelayers.0.attn_2.v_linear.bias\n",
      "decodeEncoding.decodelayers.0.attn_2.out.weight\n",
      "decodeEncoding.decodelayers.0.attn_2.out.bias\n",
      "decodeEncoding.decodelayers.0.ff.linear_1.weight\n",
      "decodeEncoding.decodelayers.0.ff.linear_1.bias\n",
      "decodeEncoding.decodelayers.0.ff.linear_2.weight\n",
      "decodeEncoding.decodelayers.0.ff.linear_2.bias\n",
      "decodeEncoding.decodelayers.1.norm_1.alpha\n",
      "decodeEncoding.decodelayers.1.norm_1.bias\n",
      "decodeEncoding.decodelayers.1.norm_2.alpha\n",
      "decodeEncoding.decodelayers.1.norm_2.bias\n",
      "decodeEncoding.decodelayers.1.norm_3.alpha\n",
      "decodeEncoding.decodelayers.1.norm_3.bias\n",
      "decodeEncoding.decodelayers.1.attn_1.q_linear.weight\n",
      "decodeEncoding.decodelayers.1.attn_1.q_linear.bias\n",
      "decodeEncoding.decodelayers.1.attn_1.k_linear.weight\n",
      "decodeEncoding.decodelayers.1.attn_1.k_linear.bias\n",
      "decodeEncoding.decodelayers.1.attn_1.v_linear.weight\n",
      "decodeEncoding.decodelayers.1.attn_1.v_linear.bias\n",
      "decodeEncoding.decodelayers.1.attn_1.out.weight\n",
      "decodeEncoding.decodelayers.1.attn_1.out.bias\n",
      "decodeEncoding.decodelayers.1.attn_2.q_linear.weight\n",
      "decodeEncoding.decodelayers.1.attn_2.q_linear.bias\n",
      "decodeEncoding.decodelayers.1.attn_2.k_linear.weight\n",
      "decodeEncoding.decodelayers.1.attn_2.k_linear.bias\n",
      "decodeEncoding.decodelayers.1.attn_2.v_linear.weight\n",
      "decodeEncoding.decodelayers.1.attn_2.v_linear.bias\n",
      "decodeEncoding.decodelayers.1.attn_2.out.weight\n",
      "decodeEncoding.decodelayers.1.attn_2.out.bias\n",
      "decodeEncoding.decodelayers.1.ff.linear_1.weight\n",
      "decodeEncoding.decodelayers.1.ff.linear_1.bias\n",
      "decodeEncoding.decodelayers.1.ff.linear_2.weight\n",
      "decodeEncoding.decodelayers.1.ff.linear_2.bias\n",
      "decodeEncoding.norm.alpha\n",
      "decodeEncoding.norm.bias\n",
      "mnm.v_r\n",
      "mnm.h_lstm\n",
      "mnm.c_lstm\n",
      "mnm.control.weight_ih\n",
      "mnm.control.weight_hh\n",
      "mnm.control.bias_ih\n",
      "mnm.control.bias_hh\n",
      "mnm.interaction.weight\n",
      "mnm.interaction.bias\n",
      "mnm.memfunc.expected_activation.weight\n",
      "mnm.memfunc.expected_activation.bias\n",
      "mnm.kv_rate.weight\n",
      "mnm.kv_rate.bias\n",
      "mnm.read_out.weight\n",
      "mnm.read_out.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can reload the notebook and reload just these saved parameters without training and the model should still exhibit the same memorization abilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bot(\n",
       "  (vocab): Vocab(\n",
       "    (embedding): Embedding(4, 32)\n",
       "    (emb2vocab): Linear(in_features=32, out_features=4, bias=False)\n",
       "  )\n",
       "  (encodeInput): Transformer(\n",
       "    (pe): PositionalEncoder(\n",
       "      (dropout): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (encodelayers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (dropout_1): Dropout(p=0.05, inplace=False)\n",
       "        (attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (norm_2): Norm()\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "        )\n",
       "        (dropout_2): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (dropout_1): Dropout(p=0.05, inplace=False)\n",
       "        (attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (norm_2): Norm()\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "        )\n",
       "        (dropout_2): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (decodelayers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.05, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.05, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): Norm()\n",
       "  )\n",
       "  (encodeEncoding): Transformer(\n",
       "    (pe): PositionalEncoder(\n",
       "      (dropout): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (encodelayers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (dropout_1): Dropout(p=0.05, inplace=False)\n",
       "        (attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (norm_2): Norm()\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "        )\n",
       "        (dropout_2): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (dropout_1): Dropout(p=0.05, inplace=False)\n",
       "        (attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (norm_2): Norm()\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "        )\n",
       "        (dropout_2): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (decodelayers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.05, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.05, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): Norm()\n",
       "  )\n",
       "  (decodeEncoding): Transformer(\n",
       "    (pe): PositionalEncoder(\n",
       "      (dropout): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (encodelayers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (dropout_1): Dropout(p=0.05, inplace=False)\n",
       "        (attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (norm_2): Norm()\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "        )\n",
       "        (dropout_2): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (dropout_1): Dropout(p=0.05, inplace=False)\n",
       "        (attn): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (norm_2): Norm()\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "        )\n",
       "        (dropout_2): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (decodelayers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.05, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.05, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): Norm()\n",
       "  )\n",
       "  (mnm): MNMp(\n",
       "    (control): LSTMCell(64, 32)\n",
       "    (interaction): Linear(in_features=32, out_features=224, bias=True)\n",
       "    (memfunc): FFMemoryLearned(\n",
       "      (expected_activation): Linear(in_features=32, out_features=96, bias=True)\n",
       "    )\n",
       "    (kv_rate): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (read_out): Linear(in_features=64, out_features=32, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab.emb2vocab.weight = pickle.load(open(\"modelstate/emb2vocab.weight.p\",\"rb\"))\n",
    "model.vocab.embedding.weight = pickle.load(open(\"modelstate/embedding.weight.p\",\"rb\"))\n",
    "model.vocab.word2index = pickle.load(open(\"modelstate/word2index.p\",\"rb\"))\n",
    "model.vocab.index2word = pickle.load(open(\"modelstate/index2word.p\",\"rb\"))\n",
    "load_model(model,\"modelstate/task.pth\")\n",
    "model.mnm.memfunc.Ws = pickle.load(open(\"modelstate/Ws.p\",\"rb\"))\n",
    "model.context_vec = pickle.load(open(\"modelstate/context_vec.p\",\"rb\"))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > my name is melissa\n",
      " > hi melissa\n",
      " \n",
      " > what is my name?\n",
      " > its melissa\n",
      " \n",
      " > my name is vicki\n",
      " > hi vicki\n",
      " \n",
      " > what is my name?\n",
      " > its vicki\n",
      " \n",
      " > my name is zen\n",
      " > hi zen\n",
      " \n",
      " > what is my name?\n",
      " > its zen\n",
      " \n",
      " > my name is sky\n",
      " > hi sky\n",
      " \n",
      " > what is my name?\n",
      " > its sky\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for tell in [\n",
    "             'my name is melissa', 'what is my name?', \n",
    "             'my name is vicki', 'what is my name?',\n",
    "             'my name is zen', 'what is my name?',\n",
    "             'my name is sky', 'what is my name?',\n",
    "             ]:\n",
    "\n",
    "    print(' > '+ tell)\n",
    "    reply = model.string2string(tell)\n",
    "    print(' > '+ reply)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
