{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.version 1.7.0\n",
      "torch.cuda.is_available() True\n",
      "torch.cuda.device_count() 2\n"
     ]
    }
   ],
   "source": [
    "import math, copy, sys, logging, json, time, random, os, string, pickle, re\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modules.TransformerComponents import Transformer\n",
    "from modules.Vocabulary import Vocab\n",
    "from modules.MetaLearnNeuralMemory import MNMp\n",
    "from modules.LoadTrainSave import save_model, load_model\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(0) \n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print('torch.version', torch.__version__)\n",
    "print('torch.cuda.is_available()', torch.cuda.is_available())\n",
    "print('torch.cuda.device_count()', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher(): \n",
    "    \n",
    "    def __init__(self, vocab):\n",
    "        \n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.vocab.string2embedding(\"my name is, hi. what is my name? its\")\n",
    "        \n",
    "        self.name_list = [\n",
    "                          'vicki', 'carson', 'melissa', 'salvador', \n",
    "                          'force', 'sky', 'zen', 'adam'\n",
    "                         ]\n",
    "\n",
    "    def random_name(self,):\n",
    "        \"\"\" Generate a random string of fixed length \"\"\"\n",
    "        return random.choice(self.name_list)\n",
    "    \n",
    "    def repeat(self, batch_size):\n",
    "        \n",
    "        self.mynameis = self.vocab.string2tensor(\"my name is\")\n",
    "        self.hi = self.vocab.string2tensor(\"hi\")\n",
    "        self.whatmyname = self.vocab.string2tensor(\"what is my name?\")\n",
    "        self.its = self.vocab.string2tensor(\"its\")\n",
    "\n",
    "        self.mynameis = self.mynameis.repeat(batch_size,1)\n",
    "        self.hi = self.hi.repeat(batch_size,1)\n",
    "        self.whatmyname = self.whatmyname.repeat(batch_size,1)\n",
    "        self.its = self.its.repeat(batch_size,1)\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        \n",
    "        self.repeat(batch_size)\n",
    "        \n",
    "        newnames = \"\"\n",
    "        for n in range(batch_size):\n",
    "            newnames += \" \" + self.random_name()\n",
    "            \n",
    "        self.vocab.string2embedding(newnames)\n",
    "        \n",
    "        self.names = self.vocab.string2tensor(newnames).T\n",
    "\n",
    "        self.intro = torch.cat((self.mynameis, self.names),dim=1)\n",
    "        self.introtarget = torch.cat((self.hi, self.names),dim=1)\n",
    "        self.yournameis = torch.cat((self.its, self.names),dim=1)\n",
    "        \n",
    "        return self.intro, self.introtarget, self.whatmyname, self.yournameis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, 'my': 4, 'name': 5, 'is': 6, ',': 7, 'hi': 8, '.': 9, 'what': 10, '?': 11, 'its': 12}\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(emb_dim=32)\n",
    "teacher = Teacher(vocab)\n",
    "\n",
    "print(vocab.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, 'my': 4, 'name': 5, 'is': 6, ',': 7, 'hi': 8, '.': 9, 'what': 10, '?': 11, 'its': 12, 'melissa': 13, 'vicki': 14, 'sky': 15, 'adam': 16}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "intro, introtarget, whatmyname, yournameis = teacher.get_batch(batch_size)\n",
    "\n",
    "print(vocab.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  5,  6, 15],\n",
       "        [ 4,  5,  6, 15],\n",
       "        [ 4,  5,  6, 14],\n",
       "        [ 4,  5,  6, 16]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro # my name is <new token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8, 15],\n",
       "        [ 8, 15],\n",
       "        [ 8, 14],\n",
       "        [ 8, 16]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "introtarget # hi <new token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10,  6,  4,  5, 11],\n",
       "        [10,  6,  4,  5, 11],\n",
       "        [10,  6,  4,  5, 11],\n",
       "        [10,  6,  4,  5, 11]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whatmyname # what is my name ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12, 15],\n",
       "        [12, 15],\n",
       "        [12, 14],\n",
       "        [12, 16]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yournameis # its <new token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bot(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim, n_layers, heads, dropout, vocab):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.sos_tok = torch.LongTensor([[self.vocab.word2index[\"<SOS>\"]]]) \n",
    "        self.eos_tok = torch.LongTensor([[self.vocab.word2index[\"<EOS>\"]]]) \n",
    "        \n",
    "        self.encodeInput = Transformer(emb_dim, n_layers, heads, dropout)\n",
    "        self.encodeEncoding = Transformer(emb_dim, n_layers, heads, dropout)\n",
    "        self.decodeEncoding = Transformer(emb_dim, n_layers, heads, dropout)\n",
    "\n",
    "        self.mnm = MNMp(emb_dim, heads)\n",
    "        \n",
    "        self.context_vec = None\n",
    "        \n",
    "    def memory_utils(self, batch_size):\n",
    "\n",
    "        if self.context_vec is None:\n",
    "            cntxt_seq_len = 1\n",
    "            self.context_vec = torch.randn(batch_size, cntxt_seq_len, self.emb_dim)\n",
    "            \n",
    "        if self.context_vec.shape[0] > batch_size:\n",
    "            self.context_vec = self.context_vec[0,:,:]\n",
    "            \n",
    "        if self.context_vec.shape[0] < batch_size:\n",
    "            self.context_vec = self.context_vec[0,:,:].repeat(batch_size, 1, 1)\n",
    "    \n",
    "        self.context_vec = self.context_vec.detach()\n",
    "        self.mnm.memfunc.detach_mem()\n",
    "        \n",
    "    def forward(self, in_toks, in_mask, out_toks, out_mask):\n",
    "        \n",
    "        self.memory_utils(batch_size = in_toks.shape[0])\n",
    "        \n",
    "        in_vecs = self.vocab.embedding(in_toks)\n",
    "        out_vec = self.vocab.embedding(out_toks)\n",
    "\n",
    "        self.context_vec, rcl, rcli = self.mnm(self.context_vec)\n",
    "        encin_vec = self.encodeInput(in_vecs, in_mask, self.context_vec, None)\n",
    "        self.context_vec = self.encodeEncoding(self.context_vec, None, encin_vec, None)\n",
    "        \n",
    "        dout = self.decodeEncoding(out_vec, out_mask, encin_vec, in_mask)\n",
    "        \n",
    "        return dout, rcl, rcli\n",
    "    \n",
    "    def teacher_forcing(self, src, trg):\n",
    "        \n",
    "        self.train()\n",
    "        trg_start = torch.cat((self.sos_tok.repeat(trg.shape[0],1), trg),dim=1)\n",
    "        trg_end = torch.cat((trg, self.eos_tok.repeat(trg.shape[0],1)),dim=1)\n",
    "        src_mask = (src != self.vocab.word2index[\"<PAD>\"]).unsqueeze(-2)\n",
    "        trg_mask = (trg_end != self.vocab.word2index[\"<PAD>\"]).unsqueeze(-2)\n",
    "        \n",
    "        seq_len = trg_start.size(1) \n",
    "        np_mask = np.triu(np.ones((1,seq_len,seq_len)),k=1).astype('uint8')\n",
    "        np_mask =  torch.from_numpy(np_mask) == 0\n",
    "        \n",
    "        if trg.is_cuda:\n",
    "            np_mask = np_mask.cuda()\n",
    "            \n",
    "        trg_mask = trg_mask & np_mask\n",
    "        \n",
    "        out_vecs, rcl, rcli = self.forward(src, src_mask, trg_start, trg_mask)\n",
    "        \n",
    "        return out_vecs, trg_end, rcl, rcli\n",
    "    \n",
    "    def string2string(self, input_string, maxlen = 20):\n",
    "        \n",
    "        self.eval()\n",
    "        in_toks = self.vocab.string2tensor(input_string)\n",
    "        in_vecs = self.vocab.embedding(in_toks)\n",
    "        \n",
    "        self.memory_utils(batch_size=in_toks.shape[0])\n",
    "        \n",
    "        self.context_vec, rcl, rcli = self.mnm(self.context_vec)\n",
    "        encin_vec = self.encodeInput(in_vecs, None, self.context_vec, None)\n",
    "        self.context_vec = self.encodeEncoding(self.context_vec, None, encin_vec, None)\n",
    "        \n",
    "        decode_toks = self.sos_tok\n",
    "        \n",
    "        for pos in range(maxlen):\n",
    "            \n",
    "            decode_vecs = self.vocab.embedding(decode_toks)\n",
    "            dout = self.decodeEncoding(decode_vecs, None, encin_vec, None)\n",
    "            vocabdist = self.vocab.emb2vocab(dout)\n",
    "            next_toks = torch.argmax(vocabdist, dim=2)\n",
    "            decode_toks = torch.cat((decode_toks, next_toks[:,-1].unsqueeze(0)), dim=1) \n",
    "            \n",
    "            if next_toks[:,-1] == self.eos_tok.squeeze(0):\n",
    "                \n",
    "                toks = decode_toks[0][1:-1].data.cpu().numpy()\n",
    "                de_str = ' '.join([self.vocab.index2word[int(tok)] for tok in toks])\n",
    "\n",
    "                return de_str\n",
    "            \n",
    "        toks = decode_toks[0].data.cpu().numpy()\n",
    "        de_str = ' '.join([self.vocab.index2word[tok] for tok in toks])\n",
    "        return de_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, 'my': 4, 'name': 5, 'is': 6, ',': 7, 'hi': 8, '.': 9, 'what': 10, '?': 11, 'its': 12}\n"
     ]
    }
   ],
   "source": [
    "emb_dim, n_layers, heads, dropout = 32, 2, 2, 0.05\n",
    "\n",
    "vocab = Vocab(emb_dim)\n",
    "model = Bot(emb_dim, n_layers, heads, dropout, vocab)\n",
    "teacher = Teacher(model.vocab)\n",
    "\n",
    "print(model.vocab.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8594 celoss 0.9942 rcloss 0.000157 d_rcloss 0.0885 training progress 0.0 learning rate [0.01]\n",
      "Saving Model...\n",
      "accuracy 0.9583 celoss 0.1105 rcloss 0.000969 d_rcloss 0.0902 training progress 0.0502 learning rate [0.009509900499]\n",
      "Saving Model...\n",
      "accuracy 0.9505 celoss 0.1363 rcloss 0.000586 d_rcloss 0.0887 training progress 0.1003 learning rate [0.009043820750088045]\n",
      "accuracy 0.9818 celoss 0.033 rcloss 0.000107 d_rcloss 0.0839 training progress 0.1505 learning rate [0.008600583546412886]\n",
      "Saving Model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7380889e80>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_batches = 64*100\n",
    "best_acc = 0\n",
    "lamda = 8\n",
    "batch_size = 128\n",
    "learning_rate = 0.01 #0.001\n",
    "\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate,betas=(0.9, 0.98),eps=1e-9)\n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min',factor=0.99,patience=50)\n",
    "\n",
    "loss_all_list = []\n",
    "rcloss_all_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "for batch in range(total_batches):\n",
    "    \n",
    "    intro, introtarget, whatmyname, yournameis = teacher.get_batch(batch_size)\n",
    "    \n",
    "    out_vecs, trg_end, rcl, rcli = model.teacher_forcing(intro, introtarget)\n",
    "    \n",
    "    vocab_logits = model.vocab.emb2vocab(out_vecs)\n",
    "    \n",
    "    predictions = vocab_logits.view(-1, vocab_logits.size(-1))\n",
    "    \n",
    "    target = trg_end.view(-1)\n",
    "\n",
    "    batch_loss = F.cross_entropy(predictions, target, \n",
    "                                 ignore_index = model.vocab.word2index[\"<PAD>\"])\n",
    "\n",
    "    reconstruction_loss = lamda*rcl\n",
    "    \n",
    "    ################# Next Part of Conversation ########################\n",
    "    \n",
    "    out_vecs, trg_end, rcl, rcli = model.teacher_forcing(whatmyname, yournameis)\n",
    "    \n",
    "    vocab_logits = model.vocab.emb2vocab(out_vecs)\n",
    "\n",
    "    predictions = vocab_logits.view(-1, vocab_logits.size(-1))\n",
    "    \n",
    "    target = trg_end.view(-1)\n",
    "    \n",
    "    acc = accuracy_score(target, torch.argmax(predictions, dim=1))\n",
    "\n",
    "    batch_loss += F.cross_entropy(predictions, target, \n",
    "                                 ignore_index = model.vocab.word2index[\"<PAD>\"])\n",
    "    \n",
    "    reconstruction_loss += lamda*rcl\n",
    "    conversation_loss = batch_loss + reconstruction_loss\n",
    "    \n",
    "    scheduler.step(conversation_loss)\n",
    "    optimizer.zero_grad()\n",
    "    conversation_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if batch % int(total_batches/20 + 1) == 0:\n",
    "        loss_all_list.append(conversation_loss.float().item())\n",
    "        rcloss_all_list.append(reconstruction_loss.float().item())\n",
    "        accuracy_list.append(acc)\n",
    "        print(\"accuracy\", round(acc,4), \n",
    "              \"celoss\", round(batch_loss.float().item(),4), \n",
    "              \"rcloss\", round(reconstruction_loss.float().item(),6), \n",
    "              \"d_rcloss\", round((rcli - rcl).float().item(),4),\n",
    "              \"training progress\", round(batch/total_batches,4),\n",
    "              \"learning rate\", scheduler._last_lr)\n",
    "        if acc > best_acc:\n",
    "            print('Saving Model...')\n",
    "            best_acc = acc\n",
    "            \n",
    "            pickle.dump(model.vocab.word2index,open(\"modelstate/word2index.p\",\"wb\"))\n",
    "            pickle.dump(model.vocab.index2word,open(\"modelstate/index2word.p\",\"wb\"))\n",
    "            pickle.dump(model.vocab.emb2vocab.weight,open(\"modelstate/emb2vocab.weight.p\",\"wb\"))\n",
    "            pickle.dump(model.vocab.embedding.weight,open(\"modelstate/embedding.weight.p\",\"wb\"))\n",
    "            pickle.dump(model.context_vec,open(\"modelstate/context_vec.p\",\"wb\"))\n",
    "            pickle.dump(model.mnm.memfunc.Ws,open(\"modelstate/Ws.p\",\"wb\"))\n",
    "            save_model(model,\"modelstate/task.pth\")\n",
    "            \n",
    "        if acc > 0.97:\n",
    "            break\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
    "fig.suptitle('Training Curves')\n",
    "ax1.set(xlabel='epochs', ylabel='train loss')\n",
    "ax2.set(xlabel='epochs', ylabel='reconstr loss')\n",
    "ax3.set(xlabel='epochs', ylabel='accuracy')\n",
    "ax1.plot(loss_all_list, label='train loss')\n",
    "ax2.plot(rcloss_all_list, label='reconstrunction loss')\n",
    "ax3.plot(accuracy_list, label='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab.embedding.weight\n",
      "vocab.emb2vocab.weight\n",
      "encodeInput.layers.0.norm_1.alpha\n",
      "encodeInput.layers.0.norm_1.bias\n",
      "encodeInput.layers.0.norm_2.alpha\n",
      "encodeInput.layers.0.norm_2.bias\n",
      "encodeInput.layers.0.norm_3.alpha\n",
      "encodeInput.layers.0.norm_3.bias\n",
      "encodeInput.layers.0.attn_1.q_linear.weight\n",
      "encodeInput.layers.0.attn_1.q_linear.bias\n",
      "encodeInput.layers.0.attn_1.k_linear.weight\n",
      "encodeInput.layers.0.attn_1.k_linear.bias\n",
      "encodeInput.layers.0.attn_1.v_linear.weight\n",
      "encodeInput.layers.0.attn_1.v_linear.bias\n",
      "encodeInput.layers.0.attn_1.out.weight\n",
      "encodeInput.layers.0.attn_1.out.bias\n",
      "encodeInput.layers.0.attn_2.q_linear.weight\n",
      "encodeInput.layers.0.attn_2.q_linear.bias\n",
      "encodeInput.layers.0.attn_2.k_linear.weight\n",
      "encodeInput.layers.0.attn_2.k_linear.bias\n",
      "encodeInput.layers.0.attn_2.v_linear.weight\n",
      "encodeInput.layers.0.attn_2.v_linear.bias\n",
      "encodeInput.layers.0.attn_2.out.weight\n",
      "encodeInput.layers.0.attn_2.out.bias\n",
      "encodeInput.layers.0.ff.linear_1.weight\n",
      "encodeInput.layers.0.ff.linear_1.bias\n",
      "encodeInput.layers.0.ff.linear_2.weight\n",
      "encodeInput.layers.0.ff.linear_2.bias\n",
      "encodeInput.layers.1.norm_1.alpha\n",
      "encodeInput.layers.1.norm_1.bias\n",
      "encodeInput.layers.1.norm_2.alpha\n",
      "encodeInput.layers.1.norm_2.bias\n",
      "encodeInput.layers.1.norm_3.alpha\n",
      "encodeInput.layers.1.norm_3.bias\n",
      "encodeInput.layers.1.attn_1.q_linear.weight\n",
      "encodeInput.layers.1.attn_1.q_linear.bias\n",
      "encodeInput.layers.1.attn_1.k_linear.weight\n",
      "encodeInput.layers.1.attn_1.k_linear.bias\n",
      "encodeInput.layers.1.attn_1.v_linear.weight\n",
      "encodeInput.layers.1.attn_1.v_linear.bias\n",
      "encodeInput.layers.1.attn_1.out.weight\n",
      "encodeInput.layers.1.attn_1.out.bias\n",
      "encodeInput.layers.1.attn_2.q_linear.weight\n",
      "encodeInput.layers.1.attn_2.q_linear.bias\n",
      "encodeInput.layers.1.attn_2.k_linear.weight\n",
      "encodeInput.layers.1.attn_2.k_linear.bias\n",
      "encodeInput.layers.1.attn_2.v_linear.weight\n",
      "encodeInput.layers.1.attn_2.v_linear.bias\n",
      "encodeInput.layers.1.attn_2.out.weight\n",
      "encodeInput.layers.1.attn_2.out.bias\n",
      "encodeInput.layers.1.ff.linear_1.weight\n",
      "encodeInput.layers.1.ff.linear_1.bias\n",
      "encodeInput.layers.1.ff.linear_2.weight\n",
      "encodeInput.layers.1.ff.linear_2.bias\n",
      "encodeInput.norm.alpha\n",
      "encodeInput.norm.bias\n",
      "encodeEncoding.layers.0.norm_1.alpha\n",
      "encodeEncoding.layers.0.norm_1.bias\n",
      "encodeEncoding.layers.0.norm_2.alpha\n",
      "encodeEncoding.layers.0.norm_2.bias\n",
      "encodeEncoding.layers.0.norm_3.alpha\n",
      "encodeEncoding.layers.0.norm_3.bias\n",
      "encodeEncoding.layers.0.attn_1.q_linear.weight\n",
      "encodeEncoding.layers.0.attn_1.q_linear.bias\n",
      "encodeEncoding.layers.0.attn_1.k_linear.weight\n",
      "encodeEncoding.layers.0.attn_1.k_linear.bias\n",
      "encodeEncoding.layers.0.attn_1.v_linear.weight\n",
      "encodeEncoding.layers.0.attn_1.v_linear.bias\n",
      "encodeEncoding.layers.0.attn_1.out.weight\n",
      "encodeEncoding.layers.0.attn_1.out.bias\n",
      "encodeEncoding.layers.0.attn_2.q_linear.weight\n",
      "encodeEncoding.layers.0.attn_2.q_linear.bias\n",
      "encodeEncoding.layers.0.attn_2.k_linear.weight\n",
      "encodeEncoding.layers.0.attn_2.k_linear.bias\n",
      "encodeEncoding.layers.0.attn_2.v_linear.weight\n",
      "encodeEncoding.layers.0.attn_2.v_linear.bias\n",
      "encodeEncoding.layers.0.attn_2.out.weight\n",
      "encodeEncoding.layers.0.attn_2.out.bias\n",
      "encodeEncoding.layers.0.ff.linear_1.weight\n",
      "encodeEncoding.layers.0.ff.linear_1.bias\n",
      "encodeEncoding.layers.0.ff.linear_2.weight\n",
      "encodeEncoding.layers.0.ff.linear_2.bias\n",
      "encodeEncoding.layers.1.norm_1.alpha\n",
      "encodeEncoding.layers.1.norm_1.bias\n",
      "encodeEncoding.layers.1.norm_2.alpha\n",
      "encodeEncoding.layers.1.norm_2.bias\n",
      "encodeEncoding.layers.1.norm_3.alpha\n",
      "encodeEncoding.layers.1.norm_3.bias\n",
      "encodeEncoding.layers.1.attn_1.q_linear.weight\n",
      "encodeEncoding.layers.1.attn_1.q_linear.bias\n",
      "encodeEncoding.layers.1.attn_1.k_linear.weight\n",
      "encodeEncoding.layers.1.attn_1.k_linear.bias\n",
      "encodeEncoding.layers.1.attn_1.v_linear.weight\n",
      "encodeEncoding.layers.1.attn_1.v_linear.bias\n",
      "encodeEncoding.layers.1.attn_1.out.weight\n",
      "encodeEncoding.layers.1.attn_1.out.bias\n",
      "encodeEncoding.layers.1.attn_2.q_linear.weight\n",
      "encodeEncoding.layers.1.attn_2.q_linear.bias\n",
      "encodeEncoding.layers.1.attn_2.k_linear.weight\n",
      "encodeEncoding.layers.1.attn_2.k_linear.bias\n",
      "encodeEncoding.layers.1.attn_2.v_linear.weight\n",
      "encodeEncoding.layers.1.attn_2.v_linear.bias\n",
      "encodeEncoding.layers.1.attn_2.out.weight\n",
      "encodeEncoding.layers.1.attn_2.out.bias\n",
      "encodeEncoding.layers.1.ff.linear_1.weight\n",
      "encodeEncoding.layers.1.ff.linear_1.bias\n",
      "encodeEncoding.layers.1.ff.linear_2.weight\n",
      "encodeEncoding.layers.1.ff.linear_2.bias\n",
      "encodeEncoding.norm.alpha\n",
      "encodeEncoding.norm.bias\n",
      "decodeEncoding.layers.0.norm_1.alpha\n",
      "decodeEncoding.layers.0.norm_1.bias\n",
      "decodeEncoding.layers.0.norm_2.alpha\n",
      "decodeEncoding.layers.0.norm_2.bias\n",
      "decodeEncoding.layers.0.norm_3.alpha\n",
      "decodeEncoding.layers.0.norm_3.bias\n",
      "decodeEncoding.layers.0.attn_1.q_linear.weight\n",
      "decodeEncoding.layers.0.attn_1.q_linear.bias\n",
      "decodeEncoding.layers.0.attn_1.k_linear.weight\n",
      "decodeEncoding.layers.0.attn_1.k_linear.bias\n",
      "decodeEncoding.layers.0.attn_1.v_linear.weight\n",
      "decodeEncoding.layers.0.attn_1.v_linear.bias\n",
      "decodeEncoding.layers.0.attn_1.out.weight\n",
      "decodeEncoding.layers.0.attn_1.out.bias\n",
      "decodeEncoding.layers.0.attn_2.q_linear.weight\n",
      "decodeEncoding.layers.0.attn_2.q_linear.bias\n",
      "decodeEncoding.layers.0.attn_2.k_linear.weight\n",
      "decodeEncoding.layers.0.attn_2.k_linear.bias\n",
      "decodeEncoding.layers.0.attn_2.v_linear.weight\n",
      "decodeEncoding.layers.0.attn_2.v_linear.bias\n",
      "decodeEncoding.layers.0.attn_2.out.weight\n",
      "decodeEncoding.layers.0.attn_2.out.bias\n",
      "decodeEncoding.layers.0.ff.linear_1.weight\n",
      "decodeEncoding.layers.0.ff.linear_1.bias\n",
      "decodeEncoding.layers.0.ff.linear_2.weight\n",
      "decodeEncoding.layers.0.ff.linear_2.bias\n",
      "decodeEncoding.layers.1.norm_1.alpha\n",
      "decodeEncoding.layers.1.norm_1.bias\n",
      "decodeEncoding.layers.1.norm_2.alpha\n",
      "decodeEncoding.layers.1.norm_2.bias\n",
      "decodeEncoding.layers.1.norm_3.alpha\n",
      "decodeEncoding.layers.1.norm_3.bias\n",
      "decodeEncoding.layers.1.attn_1.q_linear.weight\n",
      "decodeEncoding.layers.1.attn_1.q_linear.bias\n",
      "decodeEncoding.layers.1.attn_1.k_linear.weight\n",
      "decodeEncoding.layers.1.attn_1.k_linear.bias\n",
      "decodeEncoding.layers.1.attn_1.v_linear.weight\n",
      "decodeEncoding.layers.1.attn_1.v_linear.bias\n",
      "decodeEncoding.layers.1.attn_1.out.weight\n",
      "decodeEncoding.layers.1.attn_1.out.bias\n",
      "decodeEncoding.layers.1.attn_2.q_linear.weight\n",
      "decodeEncoding.layers.1.attn_2.q_linear.bias\n",
      "decodeEncoding.layers.1.attn_2.k_linear.weight\n",
      "decodeEncoding.layers.1.attn_2.k_linear.bias\n",
      "decodeEncoding.layers.1.attn_2.v_linear.weight\n",
      "decodeEncoding.layers.1.attn_2.v_linear.bias\n",
      "decodeEncoding.layers.1.attn_2.out.weight\n",
      "decodeEncoding.layers.1.attn_2.out.bias\n",
      "decodeEncoding.layers.1.ff.linear_1.weight\n",
      "decodeEncoding.layers.1.ff.linear_1.bias\n",
      "decodeEncoding.layers.1.ff.linear_2.weight\n",
      "decodeEncoding.layers.1.ff.linear_2.bias\n",
      "decodeEncoding.norm.alpha\n",
      "decodeEncoding.norm.bias\n",
      "mnm.control.weight_ih\n",
      "mnm.control.weight_hh\n",
      "mnm.control.bias_ih\n",
      "mnm.control.bias_hh\n",
      "mnm.interaction.weight\n",
      "mnm.interaction.bias\n",
      "mnm.memfunc.expected_activation.weight\n",
      "mnm.memfunc.expected_activation.bias\n",
      "mnm.kv_rate.weight\n",
      "mnm.kv_rate.bias\n",
      "mnm.read_out.weight\n",
      "mnm.read_out.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bot(\n",
       "  (vocab): Vocab(\n",
       "    (embedding): Embedding(4, 32)\n",
       "    (emb2vocab): Linear(in_features=32, out_features=4, bias=False)\n",
       "  )\n",
       "  (encodeInput): Transformer(\n",
       "    (pe): PositionalEncoder(\n",
       "      (dropout): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.05, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): TransformerLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.05, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): Norm()\n",
       "  )\n",
       "  (encodeEncoding): Transformer(\n",
       "    (pe): PositionalEncoder(\n",
       "      (dropout): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.05, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): TransformerLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.05, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): Norm()\n",
       "  )\n",
       "  (decodeEncoding): Transformer(\n",
       "    (pe): PositionalEncoder(\n",
       "      (dropout): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.05, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): TransformerLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_2): Dropout(p=0.05, inplace=False)\n",
       "        (dropout_3): Dropout(p=0.05, inplace=False)\n",
       "        (attn_1): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (attn_2): MultiHeadAttention(\n",
       "          (q_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (k_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): Norm()\n",
       "  )\n",
       "  (mnm): MNMp(\n",
       "    (control): LSTMCell(64, 32)\n",
       "    (interaction): Linear(in_features=32, out_features=224, bias=True)\n",
       "    (memfunc): FFMemoryLearned(\n",
       "      (expected_activation): Linear(in_features=32, out_features=96, bias=True)\n",
       "    )\n",
       "    (kv_rate): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (read_out): Linear(in_features=64, out_features=32, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab.emb2vocab.weight = pickle.load(open(\"modelstate/emb2vocab.weight.p\",\"rb\"))\n",
    "model.vocab.embedding.weight = pickle.load(open(\"modelstate/embedding.weight.p\",\"rb\"))\n",
    "model.vocab.word2index = pickle.load(open(\"modelstate/word2index.p\",\"rb\"))\n",
    "model.vocab.index2word = pickle.load(open(\"modelstate/index2word.p\",\"rb\"))\n",
    "load_model(model,\"modelstate/task.pth\")\n",
    "model.mnm.memfunc.Ws = pickle.load(open(\"modelstate/Ws.p\",\"rb\"))\n",
    "model.context_vec = pickle.load(open(\"modelstate/context_vec.p\",\"rb\"))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > my name is melissa\n",
      " > hi melissa\n",
      " \n",
      " > what is my name?\n",
      " > its melissa\n",
      " \n",
      " > my name is vicki\n",
      " > hi vicki\n",
      " \n",
      " > what is my name?\n",
      " > its vicki\n",
      " \n",
      " > my name is zen\n",
      " > hi zen\n",
      " \n",
      " > what is my name?\n",
      " > its zen\n",
      " \n",
      " > my name is sky\n",
      " > hi sky\n",
      " \n",
      " > what is my name?\n",
      " > its sky\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for tell in [\n",
    "             'my name is melissa', 'what is my name?', \n",
    "             'my name is vicki', 'what is my name?',\n",
    "             'my name is zen', 'what is my name?',\n",
    "             'my name is sky', 'what is my name?',\n",
    "             ]:\n",
    "\n",
    "    print(' > '+ tell)\n",
    "    reply = model.string2string(tell)\n",
    "    print(' > '+ reply)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
