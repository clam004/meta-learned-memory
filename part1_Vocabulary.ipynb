{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.version 1.7.0\n",
      "torch.cuda.is_available() True\n",
      "torch.cuda.device_count() 2\n"
     ]
    }
   ],
   "source": [
    "import math, copy, sys, logging, json, time, random, os, string, pickle, re\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(0) \n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print('torch.version', torch.__version__)\n",
    "print('torch.cuda.is_available()', torch.cuda.is_available())\n",
    "print('torch.cuda.device_count()', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Elastic Vocab Class\n",
    "\n",
    "In the next cell is the Vocb Class that we will explain.\n",
    "\n",
    "We make a few simplifying assumptions that can be modified. First we assume that the input and output vocabulary are the same, which is why we keep only one word vector embedding matrix. We also assume that the dimension of the input and output to your sequence to sequence model is the same length as the word vector length.\n",
    "\n",
    "## Embedding \n",
    "\n",
    "the embedding modules takes a token index or sequence of tokens and returns a vector or sequence of vectors. If I want to pass \"hi\" into the model as a vector, I need to pass the token index for \"hi\" into embedding. In the diagram, the token idex for \"hi\" is `3` and the `emb_dim` for our word vectors `W` is `4`. The current vocab size of the grey embedding matrix is 5 tokens, indexed 0,1,2,3,4. The blue vector is the vector for \"hi\" and is the input to the seq2seq model.\n",
    "\n",
    "<img src=\"files/Input_Elastic_Vocab.png\" height=\"1000\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \n",
    "    def __init__(self, emb_dim, word2index = None, embedding = None, \n",
    "                 word2count = None, emb2vocab = None):\n",
    "        \n",
    "        '''\n",
    "        This is a class that handles the transformation between \n",
    "        embedding space and vocabulary space. The methods of this class update both \n",
    "        the number of word embeedings and also the size of the matrix that maps\n",
    "        the vector output of a neural network module to a vector the length of\n",
    "        the output vocabulary or selecting the next token to output.\n",
    "        \n",
    "        embed_dim (integer): number of dimensions to represent words/tokens with\n",
    "        word2index (dict): a dictionary mapping a word-string to it's unique integer index \n",
    "                    that represents it in the embedding matrix\n",
    "        embedding (nn.Embedding): module that keeps a matrix where each row is a trainable\n",
    "                    word vector with the row index corresponding to the token index\n",
    "        word2count (dict): a dictionary mapping a word-string to the number of times it's been \n",
    "                    used, either in the input or the outout \n",
    "        emb2vocab (nn.Linear): module that takes an vector of length emb_dim \n",
    "                    (note: there is no reason that this has to be the same length as\n",
    "                    the word vectors) and transforms this to a vector of length\n",
    "                    vocab_size, aka self.embedding.weight.shape[0] or len(self.word2index)\n",
    "        '''\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.word2index = {\"<PAD>\":0, \"<SOS>\":1, \"<EOS>\":2, \"<UNK>\":3}\n",
    "        self.embedding = nn.Embedding(len(self.word2index), self.emb_dim)\n",
    "        self.emb2vocab = nn.Linear(self.emb_dim,len(self.word2index),bias=False)\n",
    "        self.word2count = {}\n",
    "        \n",
    "        if word2index is not None:\n",
    "            self.word2index = word2index\n",
    "            \n",
    "        if embedding is not None:\n",
    "            self.embedding = embedding\n",
    "            \n",
    "        if word2count is not None:\n",
    "            self.word2count = word2count\n",
    "            \n",
    "        if emb2vocab is not None:\n",
    "            self.emb2vocab = emb2vocab\n",
    "            \n",
    "    def string2tokens(self, string):\n",
    "        \"\"\" this function is to change the string according to \n",
    "        the substitution rules you apply below \"\"\"\n",
    "        # puts space between words and ,.!?\n",
    "        string = re.sub(r\"([,.!?])\", r\" \\1\", string) \n",
    "        # collapse words like you're and don`t, into youre and dont \n",
    "        string = re.sub(r\"([`'])\", r\"\", string) \n",
    "        # turn characters not in ^a-zA-Z0-9,.!? into a space\n",
    "        string = re.sub(r\"[^a-zA-Z0-9,.!?#]+\", r\" \", string) \n",
    "        # make all text lowercase\n",
    "        string = string.lower()\n",
    "        # split sentence string into list of word strings\n",
    "        string = string.split(\" \")\n",
    "        return string\n",
    "    \n",
    "    def tokens2tensor(self, list_o_strings):\n",
    "        \"\"\"takes a list of strings, looks each up with word2index\n",
    "           and returns a torch long tensor of those indices\"\"\"\n",
    "        integer_sequence = []\n",
    "        for wrdstr in list_o_strings:\n",
    "            if wrdstr in self.word2index:\n",
    "                integer_sequence.append(self.word2index[wrdstr])\n",
    "            else:\n",
    "                integer_sequence.append(self.word2index[\"<UNK>\"])\n",
    "        return torch.LongTensor([integer_sequence])\n",
    "    \n",
    "    def string2tensor(self, sentence_str):\n",
    "        \"\"\"takes string sentence, returns tensor integer sentence\n",
    "        without adding new words to the vocabulary\"\"\"\n",
    "        list_o_strings = self.string2tokens(sentence_str)\n",
    "        tnsr_int_sntnc = self.tokens2tensor(list_o_strings)\n",
    "        return tnsr_int_sntnc\n",
    "    \n",
    "    def string2embedding(self, sentence_str):\n",
    "        \"\"\"takes a sentence as a string and increments the\n",
    "        wordcount of each word in the string, if word has never been\n",
    "        seen, it is added to the word2index and embedding\"\"\"\n",
    "        list_o_strings = self.string2tokens(sentence_str)\n",
    "        for wrdstr in list_o_strings:\n",
    "            if wrdstr in self.word2index and wrdstr in self.word2count:\n",
    "                self.word2count[wrdstr] += 1\n",
    "            else:\n",
    "                self.word2count[wrdstr] = 1\n",
    "                self.word2index[wrdstr] = len(self.word2index)\n",
    "                self.embedding.weight = nn.Parameter(torch.cat((self.embedding.weight, \n",
    "                     torch.randn(1, self.emb_dim)), dim=0), requires_grad=True)\n",
    "                self.emb2vocab.weight = nn.Parameter(torch.cat((self.emb2vocab.weight,\n",
    "                     torch.randn(1, self.emb_dim)), dim=0), requires_grad=True)\n",
    "        return list_o_strings\n",
    "    \n",
    "    def string2embed2tensor(self, sentence_str):\n",
    "        \"\"\" takes string sentence, returns tensor integer sentence\n",
    "         adding new words to the vocabulary\"\"\"\n",
    "        \n",
    "        list_o_strings = self.string2embedding(sentence_str)\n",
    "        tnsr_int_sntnc = self.tokens2tensor(list_o_strings)\n",
    "        return tnsr_int_sntnc\n",
    "    \n",
    "    def prunevocab(self, mincount):\n",
    "        \"\"\" loops through word2count to find words used less than mincount\n",
    "        if it's count is less than mincount then its index is used to remove\n",
    "        the row of that index in the embedding matrix. The word2index dictionary\n",
    "        is also adjusted by decrementing it's integer indices to make restore the\n",
    "        word-index-vector relationship. lastly the words are removed from word2index\n",
    "        and word2count \"\"\"\n",
    "        words2del = [] \n",
    "        for wrdstr in self.word2count:\n",
    "            if self.word2count[wrdstr] < mincount:\n",
    "                words2del.append(wrdstr)\n",
    "                wrdidx = self.word2index[wrdstr]\n",
    "                self.embedding.weight=nn.Parameter(torch.cat((self.embedding.weight[:wrdidx], \n",
    "                                 self.embedding.weight[wrdidx+1:]), dim=0),requires_grad=True)\n",
    "                self.emb2vocab.weight=nn.Parameter(torch.cat((self.emb2vocab.weight[:wrdidx], \n",
    "                                 self.emb2vocab.weight[wrdidx+1:]),dim=0),requires_grad=True)\n",
    "                for decrwrd in self.word2count:\n",
    "                    self.word2index[decrwrd] -= 1\n",
    "\n",
    "        for wrd in words2del:\n",
    "            del self.word2index[wrd]\n",
    "            del self.word2count[wrd]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a Vocabulary\n",
    "\n",
    "For simplicity we assume that the vector output of our neural network will be the same length as the word vectors\n",
    "\n",
    "Notice that there are 4 initial tokens: padding, start of sentence, end of sentence and unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.1258, -1.1524, -0.2506, -0.4339],\n",
      "        [ 0.8487,  0.6920, -0.3160, -2.1152],\n",
      "        [ 0.3223, -1.2633,  0.3500,  0.3081],\n",
      "        [ 0.1198,  1.2377,  1.1168, -0.2473]], requires_grad=True)\n",
      " \n",
      "Parameter containing:\n",
      "tensor([[ 0.1977,  0.3000, -0.3390, -0.2177],\n",
      "        [ 0.1816,  0.4152, -0.1029,  0.3742],\n",
      "        [-0.0806,  0.0529,  0.4527, -0.4638],\n",
      "        [-0.3148, -0.1266, -0.1949,  0.4320]], requires_grad=True)\n",
      " \n",
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
      " \n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(emb_dim = 4)\n",
    "\n",
    "print(vocab.embedding.weight)\n",
    "print(' ')\n",
    "print(vocab.emb2vocab.weight)\n",
    "print(' ')\n",
    "print(vocab.word2index)\n",
    "print(' ')\n",
    "print(vocab.word2count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate Input Vocab Functions\n",
    "\n",
    "We will start by inputting a sentence into the Vocab\n",
    "\n",
    "A. \n",
    "\n",
    "We use the string2token function to tokenize the sentence into a\n",
    "list of strings\n",
    "\n",
    "All the tokens in this string are unknown to our vocabulary at first, which is defined as whether or not the string has a key for it in the word2str dictionary\n",
    "\n",
    "Since each string token is not known, the token indices returned by the tokens2tensor\n",
    "function are all the token index for unknown token `<UNK>`, Note that this is a tensor of indices, NOT a tensor of word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', '!', 'youre', 'coming', 'tonight', 'right', '?', '!']\n",
      "tensor([[3, 3, 3, 3, 3, 3, 3, 3]])\n"
     ]
    }
   ],
   "source": [
    "list_o_strings = vocab.string2tokens(\"hi![] you're coming TONIGHT right?!\")\n",
    "\n",
    "print(list_o_strings)\n",
    "\n",
    "tensorsentence = vocab.tokens2tensor(list_o_strings)\n",
    "\n",
    "print(tensorsentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B.\n",
    "\n",
    "`string2embedding` will add these new tokens to both the vocabulary embedding matrix and also the embedding to vocab matrix by adding a row vector to each for every new word. The growing of embedding and emb2vocab allow access to thesenew tokens as an input and output respectively by the seq2seq model\n",
    "\n",
    "The diagram shows growing of a matrix, this growing process applies both to the `vocab.embedding.weight` and `vocab.emb2vocab.weight` matrices.\n",
    "\n",
    "<img src=\"files/Expand_Elastic_Vocab.png\" height=\"1000\" width=\"800\" />\n",
    "\n",
    "The diagram shows an example of growing the matrix by one token only, the vocab size expands from 5 tokens to 6, the emb_dim is still the same at 4 dimensions. V' is the new Vocab size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', '!', 'youre', 'coming', 'tonight', 'right', '?', '!']\n",
      "tensor([[ 4,  5,  6,  7,  8,  9, 10,  5]])\n"
     ]
    }
   ],
   "source": [
    "list_o_strings = vocab.string2embedding(\"hi![] you're coming TONIGHT right?!\")\n",
    "\n",
    "print(list_o_strings)\n",
    "\n",
    "tensorsentence = vocab.tokens2tensor(list_o_strings)\n",
    "\n",
    "print(tensorsentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.1258, -1.1524, -0.2506, -0.4339],\n",
      "        [ 0.8487,  0.6920, -0.3160, -2.1152],\n",
      "        [ 0.3223, -1.2633,  0.3500,  0.3081],\n",
      "        [ 0.1198,  1.2377,  1.1168, -0.2473],\n",
      "        [-0.5663,  0.3731, -0.8920, -1.5091],\n",
      "        [ 0.1919,  1.2638, -1.2904, -0.7911],\n",
      "        [ 0.1920,  0.5428, -2.2188,  0.2590],\n",
      "        [-0.0404,  0.2881, -0.0075, -0.9145],\n",
      "        [ 2.0576, -0.0354,  0.0627, -0.7663],\n",
      "        [-1.5055, -0.6610,  1.3232,  0.0371],\n",
      "        [-0.4584, -0.3360, -1.5700,  1.2315]], requires_grad=True)\n",
      " \n",
      "Parameter containing:\n",
      "tensor([[ 0.1977,  0.3000, -0.3390, -0.2177],\n",
      "        [ 0.1816,  0.4152, -0.1029,  0.3742],\n",
      "        [-0.0806,  0.0529,  0.4527, -0.4638],\n",
      "        [-0.3148, -0.1266, -0.1949,  0.4320],\n",
      "        [ 0.3704,  1.4565,  0.9398,  0.7748],\n",
      "        [-0.0209, -0.7185,  0.5186, -1.3125],\n",
      "        [-1.0297, -0.5008,  0.2734, -0.9181],\n",
      "        [-1.0886, -0.2666,  0.1894, -0.2190],\n",
      "        [ 1.0993,  2.7565,  0.1753, -0.9315],\n",
      "        [-0.2849, -0.1334,  1.8929,  3.1110],\n",
      "        [ 1.3946,  1.1711,  0.4335, -1.7343]], requires_grad=True)\n",
      " \n",
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, 'hi': 4, '!': 5, 'youre': 6, 'coming': 7, 'tonight': 8, 'right': 9, '?': 10}\n",
      " \n",
      "{'hi': 1, '!': 2, 'youre': 1, 'coming': 1, 'tonight': 1, 'right': 1, '?': 1}\n"
     ]
    }
   ],
   "source": [
    "# There are new rows added to the embedding matrix and to the emb2vocab matrix\n",
    "\n",
    "print(vocab.embedding.weight)\n",
    "print(' ')\n",
    "print(vocab.emb2vocab.weight)\n",
    "print(' ')\n",
    "print(vocab.word2index)\n",
    "print(' ')\n",
    "print(vocab.word2count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate Output functions\n",
    "\n",
    "I am going to make a random vector `seq2seq_ouput`, pretend this is the output from our seq2seq model.\n",
    "\n",
    "I am going to pass this vector to emb2vocab to give us our `token_selection_vector`, which should be the same length as the length of our vocabulary. \n",
    "\n",
    "In the diagram `vocab.emb2vocab.weight` is in green, `seq2seq_ouput` is our red vector, which by matrix multiplication `emb2vocab` transforms into `token_selection_vector` in purple, which is placed through a softmax can be interpreted as a probability distribution over all the tokens of our vocabulary, yellow.\n",
    "\n",
    "<img src=\"files/Output_Elastic_Vocab.png\" height=\"1000\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9759,  0.7112, -0.7582, -0.6436]])\n",
      "tensor([[ 0.4176, -0.0447,  0.0715,  0.0869, -0.5368, -0.0392,  1.0324,  0.8701,\n",
      "          1.3544, -3.2543,  0.2593]], grad_fn=<MmBackward>)\n",
      " \n",
      "torch.Size([1, 11]) 11\n"
     ]
    }
   ],
   "source": [
    "seq2seq_ouput = torch.randn((1,4))\n",
    "\n",
    "print(seq2seq_ouput)\n",
    "\n",
    "token_selection_vector = vocab.emb2vocab(seq2seq_ouput)\n",
    "\n",
    "print(token_selection_vector)\n",
    "\n",
    "print(' ')\n",
    "\n",
    "print(token_selection_vector.shape, len(vocab.word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
