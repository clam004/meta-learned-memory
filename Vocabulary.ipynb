{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "imperial-stranger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "torch.version 1.7.1\n",
      "torch.cuda.is_available() False\n",
      "torch.cuda.device_count() 0\n"
     ]
    }
   ],
   "source": [
    "import math, copy, sys, logging, json, time, random, os, string, pickle, re\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(0) \n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print('torch.version', torch.__version__)\n",
    "print('torch.cuda.is_available()', torch.cuda.is_available())\n",
    "print('torch.cuda.device_count()', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "killing-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \n",
    "    def __init__(self, emb_dim, word2index = None, embedding = None, \n",
    "                 word2count = None, emb2vocab = None):\n",
    "        \n",
    "        '''\n",
    "        embed_dim: integer, number of dimensions to represent words/tokens with\n",
    "        word2index: a dictionary mapping a word-string to it's unique integer index \n",
    "                    that represents it in the embedding matrix\n",
    "        word2count: a dictionary mapping a word-string to the number of times it's been \n",
    "                    used, either in the input or the outout \n",
    "        '''\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.word2index = {\"<PAD>\":0, \"<SOS>\":1, \"<EOS>\":2, \"<UNK>\":3}\n",
    "        self.embedding = nn.Embedding(len(self.word2index), self.emb_dim)\n",
    "        self.emb2vocab=nn.Linear(self.emb_dim,len(self.word2index),bias=False)\n",
    "        self.word2count = {}\n",
    "        \n",
    "        if word2index is not None:\n",
    "            self.word2index = word2index\n",
    "            \n",
    "        if embedding is not None:\n",
    "            self.embedding = embedding\n",
    "            \n",
    "        if word2count is not None:\n",
    "            self.word2count = word2count\n",
    "            \n",
    "        if emb2vocab is not None:\n",
    "            self.emb2vocab = emb2vocab\n",
    "            \n",
    "    def string2tokens(self, string):\n",
    "        \"\"\" this function is to change the string according to \n",
    "        the substitution rules you apply below \"\"\"\n",
    "        # puts space between words and ,.!?\n",
    "        string = re.sub(r\"([,.!?])\", r\" \\1\", string) \n",
    "        # collapse words like you're and don`t, into youre and dont \n",
    "        string = re.sub(r\"([`'])\", r\"\", string) \n",
    "        # turn characters not in ^a-zA-Z0-9,.!? into a space\n",
    "        string = re.sub(r\"[^a-zA-Z0-9,.!?#]+\", r\" \", string) \n",
    "        # make all text lowercase\n",
    "        string = string.lower()\n",
    "        # split sentence string into list of word strings\n",
    "        string = string.split(\" \")\n",
    "        return string\n",
    "    \n",
    "    def tokens2tensor(self, list_o_strings):\n",
    "        \"\"\"takes a list of strings, looks each up with word2index\n",
    "           and returns a torch long tensor of those indices\"\"\"\n",
    "        integer_sequence = []\n",
    "        for wrdstr in list_o_strings:\n",
    "            if wrdstr in self.word2index:\n",
    "                integer_sequence.append(self.word2index[wrdstr])\n",
    "            else:\n",
    "                integer_sequence.append(self.word2index[\"<UNK>\"])\n",
    "        return torch.LongTensor([integer_sequence])\n",
    "    \n",
    "    def string2tensor(self, sentence_str):\n",
    "        \"\"\" takes string sentence, returns tensor integer sentence\n",
    "        without adding new words to the vocabulary\"\"\"\n",
    "        list_o_strings = self.string2tokens(sentence_str)\n",
    "        tnsr_int_sntnc = self.tokens2tensor(list_o_strings)\n",
    "        return tnsr_int_sntnc\n",
    "    \n",
    "    def string2embedding(self, sentence_str):\n",
    "        \"\"\"takes a sentence as a string and increments the\n",
    "        wordcount of each word in the string, if word has never been\n",
    "        seen, it is added to the word2index and embedding\"\"\"\n",
    "        list_o_strings = self.string2tokens(sentence_str)\n",
    "        for wrdstr in list_o_strings:\n",
    "            if wrdstr in self.word2index and wrdstr in self.word2count:\n",
    "                self.word2count[wrdstr] += 1\n",
    "            else:\n",
    "                self.word2count[wrdstr] = 1\n",
    "                self.word2index[wrdstr] = len(self.word2index)\n",
    "                self.embedding.weight = nn.Parameter(torch.cat((self.embedding.weight, \n",
    "                     torch.randn(1, self.emb_dim)), dim=0), requires_grad=True)\n",
    "                self.emb2vocab.weight = nn.Parameter(torch.cat((self.emb2vocab.weight,\n",
    "                     torch.randn(1, self.emb_dim)), dim=0), requires_grad=True)\n",
    "        return list_o_strings\n",
    "    \n",
    "    def string2embed2tensor(self, sentence_str):\n",
    "        \"\"\" takes string sentence, returns tensor integer sentence\n",
    "         adding new words to the vocabulary\"\"\"\n",
    "        \n",
    "        list_o_strings = self.string2embedding(sentence_str)\n",
    "        tnsr_int_sntnc = self.tokens2tensor(list_o_strings)\n",
    "        return tnsr_int_sntnc\n",
    "    \n",
    "    def prunevocab(self, mincount):\n",
    "        \"\"\" loops through word2count to find words used less than mincount\n",
    "        if it's count is less than mincount then its index is used to remove\n",
    "        the row of that index in the embedding matrix. The word2index dictionary\n",
    "        is also adjusted by decrementing it's integer indices to make restore the\n",
    "        word-index-vector relationship. lastly the words are removed from word2index\n",
    "        and word2count \"\"\"\n",
    "        words2del = [] \n",
    "        for wrdstr in self.word2count:\n",
    "            if self.word2count[wrdstr] < mincount:\n",
    "                words2del.append(wrdstr)\n",
    "                wrdidx = self.word2index[wrdstr]\n",
    "                self.embedding.weight=nn.Parameter(torch.cat((self.embedding.weight[:wrdidx], \n",
    "                                 self.embedding.weight[wrdidx+1:]), dim=0),requires_grad=True)\n",
    "                self.emb2vocab.weight=nn.Parameter(torch.cat((self.emb2vocab.weight[:wrdidx], \n",
    "                                 self.emb2vocab.weight[wrdidx+1:]),dim=0),requires_grad=True)\n",
    "                for decrwrd in self.word2count:\n",
    "                    self.word2index[decrwrd] -= 1\n",
    "\n",
    "        for wrd in words2del:\n",
    "            del self.word2index[wrd]\n",
    "            del self.word2count[wrd]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-alignment",
   "metadata": {},
   "source": [
    "## initialize a vocabulary\n",
    "\n",
    "notice that there are 4 initial tokens: padding, start of sentence, end of sentence and unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "handled-tsunami",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.1258, -1.1524, -0.2506, -0.4339],\n",
      "        [ 0.8487,  0.6920, -0.3160, -2.1152],\n",
      "        [ 0.3223, -1.2633,  0.3500,  0.3081],\n",
      "        [ 0.1198,  1.2377,  1.1168, -0.2473]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1977,  0.3000, -0.3390, -0.2177],\n",
      "        [ 0.1816,  0.4152, -0.1029,  0.3742],\n",
      "        [-0.0806,  0.0529,  0.4527, -0.4638],\n",
      "        [-0.3148, -0.1266, -0.1949,  0.4320]], requires_grad=True)\n",
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(emb_dim = 4)\n",
    "print(vocab.embedding.weight)\n",
    "print(vocab.emb2vocab.weight)\n",
    "print(vocab.word2index)\n",
    "print(vocab.word2count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "patient-activation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', '!', 'youre', 'coming', 'tonight', 'right', '?', '!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of the tokenizer function\n",
    "list_o_strings = vocab.string2tokens(\"hi![] you're coming TONIGHT right?!\")\n",
    "print(list_o_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-revision",
   "metadata": {},
   "source": [
    "All the tokens in this string are unknown to our vocabulary at first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "preceding-socket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 3, 3, 3, 3, 3, 3, 3]])\n"
     ]
    }
   ],
   "source": [
    "tensorsentence = vocab.tokens2tensor(list_o_strings)\n",
    "print(tensorsentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-president",
   "metadata": {},
   "source": [
    "add a sentence to the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "integral-alignment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', '!', 'youre', 'coming', 'tonight', 'right', '?', '!']\n",
      "tensor([[ 4,  5,  6,  7,  8,  9, 10,  5]])\n"
     ]
    }
   ],
   "source": [
    "# example of the tokenizer function\n",
    "list_o_strings = vocab.string2embedding(\"hi![] you're coming TONIGHT right?!\")\n",
    "print(list_o_strings)\n",
    "tensorsentence = vocab.tokens2tensor(list_o_strings)\n",
    "print(tensorsentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-counter",
   "metadata": {},
   "source": [
    "<img src='files/elastic_vocab.jpg' height=800 width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-earth",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
