{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "torch.version 1.7.0\n",
      "torch.cuda.is_available() True\n",
      "torch.cuda.device_count() 2\n"
     ]
    }
   ],
   "source": [
    "import math, copy, sys, logging, json, time, random, os, string, pickle, re\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modules.TransformerComponents import Transformer\n",
    "from modules.Vocabulary import Vocab\n",
    "from modules.MetaLearnNeuralMemory import FFMemoryLearned\n",
    "from modules.LoadTrainSave import save_model, load_model, Teacher\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(0) \n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print('torch.version', torch.__version__)\n",
    "print('torch.cuda.is_available()', torch.cuda.is_available())\n",
    "print('torch.cuda.device_count()', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNMp(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_hidden, n_heads = 4):\n",
    "        \n",
    "        \"\"\" dim_hidden is the hidden size of the LSTM controller,\n",
    "            the Memory Network, and the interaction vectors\n",
    "            n_heads is the number of interaction heads \"\"\"\n",
    "\n",
    "        super(MNMp, self).__init__()\n",
    "        \n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.control = nn.LSTMCell(dim_hidden*2, dim_hidden)\n",
    "        \n",
    "        dim_concat_interact = dim_hidden*n_heads*3 + dim_hidden\n",
    "        self.interaction = nn.Linear(dim_hidden, dim_concat_interact)\n",
    "        self.memfunc = FFMemoryLearned(dim_hidden)\n",
    "        self.kv_rate = nn.Linear(dim_hidden, 1)\n",
    "        self.read_out = nn.Linear(dim_hidden+dim_hidden, dim_hidden)\n",
    "        \n",
    "        #self.v_r = None\n",
    "        #self.h_lstm = None\n",
    "        #self.c_lstm = None\n",
    "        \n",
    "        #''' \n",
    "        self.v_r = torch.nn.Parameter( torch.zeros((1, self.dim_hidden)))\n",
    "        self.h_lstm = torch.nn.Parameter(torch.zeros((1, self.dim_hidden)))\n",
    "        self.c_lstm = torch.nn.Parameter(torch.zeros((1, self.dim_hidden)))\n",
    "        #'''\n",
    "        \n",
    "    def repeat_v_h_c(self, batch_size):\n",
    "\n",
    "            #self.v_r = torch.zeros((batch_size, self.dim_hidden)).float()\n",
    "            #self.h_lstm = torch.zeros((batch_size, self.dim_hidden)).float()\n",
    "            #self.c_lstm = torch.zeros((batch_size, self.dim_hidden)).float()\n",
    "            \n",
    "            #'''\n",
    "            if self.v_r.shape[0] != batch_size:\n",
    "                v_r = self.v_r.repeat(batch_size,1)\n",
    "                h_lstm = self.h_lstm.repeat(batch_size,1)\n",
    "                c_lstm = self.c_lstm.repeat(batch_size,1)\n",
    "            #'''\n",
    "            \n",
    "            if next(self.parameters()).is_cuda:\n",
    "                self.v_r = self.v_r.cuda()\n",
    "                self.h_lstm = self.h_lstm.cuda()\n",
    "                self.c_lstm = self.c_lstm.cuda()\n",
    "                                         \n",
    "            return v_r, h_lstm, c_lstm\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \"\"\" the input must have shape (batch_size, emb_dim) because it will be \n",
    "        concatenated with self.v_r of the same shape \"\"\"\n",
    "\n",
    "        v_r, h_lstm, c_lstm = self.repeat_v_h_c(x.shape[0])\n",
    "        \n",
    "        x = x.squeeze(1)\n",
    "        \n",
    "        h_lstm, c_lstm = self.control(torch.cat([x, v_r], dim=1), (h_lstm, c_lstm))\n",
    "        \n",
    "        int_vecs = torch.tanh(self.interaction(h_lstm))\n",
    "        \n",
    "        beta_, n_k_v = torch.split(int_vecs, \n",
    "                                   [self.dim_hidden,self.dim_hidden*self.n_heads*3],\n",
    "                                   dim=1)  \n",
    "        \n",
    "        beta = torch.sigmoid(self.kv_rate(beta_)) #(batch_size,1)\n",
    "        \n",
    "        n_k_v = n_k_v.view(n_k_v.shape[0], self.n_heads, -1).contiguous()\n",
    "        \n",
    "        k_w, v_w, k_r = torch.chunk(n_k_v, 3, dim=2)\n",
    "        \n",
    "        reconst_loss, reconst_loss_init = self.memfunc.update(k_w, v_w, beta_rate=beta)\n",
    "        \n",
    "        v_r = self.memfunc.read(k_r)\n",
    "        \n",
    "        h_lstm = self.read_out(torch.cat([h_lstm, v_r], dim=1))\n",
    "\n",
    "        return h_lstm.unsqueeze(1), reconst_loss, reconst_loss_init "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Bot(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim, n_layers, heads, dropout, vocab):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.sos_tok = torch.LongTensor([[self.vocab.word2index[\"<SOS>\"]]]) \n",
    "        self.eos_tok = torch.LongTensor([[self.vocab.word2index[\"<EOS>\"]]]) \n",
    "        \n",
    "        self.encodeInput = Transformer(emb_dim, n_layers, heads, dropout)\n",
    "        self.encodeEncoding = Transformer(emb_dim, n_layers, heads, dropout)\n",
    "        self.decodeEncoding = Transformer(emb_dim, n_layers, heads, dropout)\n",
    "\n",
    "        self.mnm = MNMp(emb_dim, heads)\n",
    "        \n",
    "        self.context_vec = None\n",
    "        \n",
    "    def memory_utils(self, batch_size):\n",
    "\n",
    "        if self.context_vec is None:\n",
    "            cntxt_seq_len = 1\n",
    "            self.context_vec = torch.randn(batch_size, cntxt_seq_len, self.emb_dim)\n",
    "            \n",
    "        if self.context_vec.shape[0] > batch_size:\n",
    "            self.context_vec = self.context_vec[0,:,:]\n",
    "            \n",
    "        if self.context_vec.shape[0] < batch_size:\n",
    "            self.context_vec = self.context_vec[0,:,:].repeat(batch_size, 1, 1)\n",
    "    \n",
    "        self.context_vec = self.context_vec.detach()\n",
    "        self.mnm.memfunc.detach_mem()\n",
    "        \n",
    "    def forward(self, in_toks, in_mask, out_toks, out_mask):\n",
    "        \n",
    "        self.memory_utils(batch_size = in_toks.shape[0])\n",
    "        \n",
    "        in_vecs = self.vocab.embedding(in_toks)\n",
    "        out_vec = self.vocab.embedding(out_toks)\n",
    "\n",
    "        self.context_vec, rcl, rcli = self.mnm(self.context_vec)\n",
    "        encin_vec = self.encodeInput(in_vecs, in_mask, self.context_vec, None)\n",
    "        self.context_vec = self.encodeEncoding(self.context_vec, None, encin_vec, None)\n",
    "        \n",
    "        dout = self.decodeEncoding(out_vec, out_mask, encin_vec, in_mask)\n",
    "        \n",
    "        return dout, rcl, rcli\n",
    "    \n",
    "    def teacher_forcing(self, src, trg):\n",
    "        \n",
    "        self.train()\n",
    "        trg_start = torch.cat((self.sos_tok.repeat(trg.shape[0],1), trg),dim=1)\n",
    "        trg_end = torch.cat((trg, self.eos_tok.repeat(trg.shape[0],1)),dim=1)\n",
    "        src_mask = (src != self.vocab.word2index[\"<PAD>\"]).unsqueeze(-2)\n",
    "        trg_mask = (trg_end != self.vocab.word2index[\"<PAD>\"]).unsqueeze(-2)\n",
    "        \n",
    "        seq_len = trg_start.size(1) \n",
    "        np_mask = np.triu(np.ones((1,seq_len,seq_len)),k=1).astype('uint8')\n",
    "        np_mask =  torch.from_numpy(np_mask) == 0\n",
    "        \n",
    "        if trg.is_cuda:\n",
    "            np_mask = np_mask.cuda()\n",
    "            \n",
    "        trg_mask = trg_mask & np_mask\n",
    "        \n",
    "        out_vecs, rcl, rcli = self.forward(src, src_mask, trg_start, trg_mask)\n",
    "        \n",
    "        return out_vecs, trg_end, rcl, rcli\n",
    "    \n",
    "    def string2string(self, input_string, maxlen = 20):\n",
    "        \n",
    "        self.eval()\n",
    "        in_toks = self.vocab.string2tensor(input_string)\n",
    "        in_vecs = self.vocab.embedding(in_toks)\n",
    "        \n",
    "        self.memory_utils(batch_size=in_toks.shape[0])\n",
    "        \n",
    "        self.context_vec, rcl, rcli = self.mnm(self.context_vec)\n",
    "        encin_vec = self.encodeInput(in_vecs, None, self.context_vec, None)\n",
    "        self.context_vec = self.encodeEncoding(self.context_vec, None, encin_vec, None)\n",
    "        \n",
    "        decode_toks = self.sos_tok\n",
    "        \n",
    "        for pos in range(maxlen):\n",
    "            \n",
    "            decode_vecs = self.vocab.embedding(decode_toks)\n",
    "            dout = self.decodeEncoding(decode_vecs, None, encin_vec, None)\n",
    "            vocabdist = self.vocab.emb2vocab(dout)\n",
    "            next_toks = torch.argmax(vocabdist, dim=2)\n",
    "            decode_toks = torch.cat((decode_toks, next_toks[:,-1].unsqueeze(0)), dim=1) \n",
    "            \n",
    "            if next_toks[:,-1] == self.eos_tok.squeeze(0):\n",
    "                \n",
    "                toks = decode_toks[0][1:-1].data.cpu().numpy()\n",
    "                de_str = ' '.join([self.vocab.index2word[int(tok)] for tok in toks])\n",
    "\n",
    "                return de_str\n",
    "            \n",
    "        toks = decode_toks[0].data.cpu().numpy()\n",
    "        de_str = ' '.join([self.vocab.index2word[tok] for tok in toks])\n",
    "        return de_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim, n_layers, heads, dropout = 32, 2, 2, 0.05\n",
    "\n",
    "vocab = Vocab(emb_dim)\n",
    "model = Bot(emb_dim, n_layers, heads, dropout, vocab)\n",
    "teacher = Teacher(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model...\n",
      "mean accuracy 0.7031 celoss 0.7518 rcloss 7.6e-05 d_rcloss 0.0424 training progress 0.0 learning rate [0.01]\n",
      "Saving Model...\n",
      "mean accuracy 0.7422 celoss 0.4669 rcloss 0.004161 d_rcloss 0.0468 training progress 0.0516 learning rate [0.01]\n",
      "Saving Model...\n",
      "mean accuracy 0.7639 celoss 0.5378 rcloss 0.006587 d_rcloss 0.043 training progress 0.1031 learning rate [0.01]\n",
      "Saving Model...\n",
      "mean accuracy 0.7682 celoss 0.4938 rcloss 0.000683 d_rcloss 0.0445 training progress 0.1547 learning rate [0.01]\n",
      "mean accuracy 0.7667 celoss 0.4806 rcloss 0.001113 d_rcloss 0.0479 training progress 0.2062 learning rate [0.01]\n",
      "Saving Model...\n",
      "mean accuracy 0.7752 celoss 0.4967 rcloss 0.002815 d_rcloss 0.0473 training progress 0.2578 learning rate [0.01]\n",
      "Saving Model...\n",
      "mean accuracy 0.7783 celoss 0.4513 rcloss 0.006464 d_rcloss 0.0531 training progress 0.3094 learning rate [0.01]\n",
      "Saving Model...\n",
      "mean accuracy 0.7812 celoss 0.4197 rcloss 0.001991 d_rcloss 0.0513 training progress 0.3609 learning rate [0.01]\n",
      "Saving Model...\n",
      "mean accuracy 0.7882 celoss 0.3193 rcloss 0.002479 d_rcloss 0.0554 training progress 0.4125 learning rate [0.01]\n",
      "Saving Model...\n",
      "mean accuracy 0.7969 celoss 0.2916 rcloss 0.001069 d_rcloss 0.0556 training progress 0.4641 learning rate [0.01]\n",
      "Saving Model...\n",
      "mean accuracy 0.813 celoss 0.3721 rcloss 0.003536 d_rcloss 0.0557 training progress 0.5156 learning rate [0.01]\n",
      "Saving Model...\n",
      "mean accuracy 0.8229 celoss 0.3381 rcloss 0.001593 d_rcloss 0.0585 training progress 0.5672 learning rate [0.01]\n",
      "Saving Model...\n",
      "mean accuracy 0.8333 celoss 0.2342 rcloss 0.001854 d_rcloss 0.0611 training progress 0.6188 learning rate [0.01]\n",
      "Saving Model...\n",
      "mean accuracy 0.851 celoss 0.1711 rcloss 0.000771 d_rcloss 0.0592 training progress 0.6703 learning rate [0.01]\n",
      "Saving Model...\n",
      "mean accuracy 0.8677 celoss 0.2241 rcloss 0.000831 d_rcloss 0.0592 training progress 0.7219 learning rate [0.01]\n",
      "Saving Model...\n",
      "mean accuracy 0.8823 celoss 0.1624 rcloss 0.000777 d_rcloss 0.0618 training progress 0.7734 learning rate [0.01]\n",
      "Saving Model...\n",
      "mean accuracy 0.8979 celoss 0.1414 rcloss 0.001022 d_rcloss 0.0614 training progress 0.825 learning rate [0.01]\n",
      "Saving Model...\n",
      "mean accuracy 0.912 celoss 0.2087 rcloss 0.001037 d_rcloss 0.0646 training progress 0.8766 learning rate [0.01]\n",
      "Saving Model...\n",
      "mean accuracy 0.9245 celoss 0.1019 rcloss 0.001436 d_rcloss 0.0594 training progress 0.9281 learning rate [0.01]\n",
      "Saving Model...\n",
      "mean accuracy 0.9318 celoss 0.1706 rcloss 0.001579 d_rcloss 0.0632 training progress 0.9797 learning rate [0.01]\n"
     ]
    }
   ],
   "source": [
    "total_batches = 64*10\n",
    "best_acc = 0\n",
    "lamda = 8\n",
    "batch_size = 64\n",
    "learning_rate = 0.01 #0.001\n",
    "\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate,betas=(0.9, 0.98),eps=1e-9)\n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min',factor=0.99,patience=100)\n",
    "\n",
    "loss_all_list = []\n",
    "rcloss_all_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "for batch in range(total_batches):\n",
    "    \n",
    "    intro, introtarget, whatmyname, yournameis = teacher.get_batch(batch_size)\n",
    "    \n",
    "    out_vecs, trg_end, rcl, rcli = model.teacher_forcing(intro, introtarget)\n",
    "    \n",
    "    vocab_logits = model.vocab.emb2vocab(out_vecs)\n",
    "    \n",
    "    predictions = vocab_logits.view(-1, vocab_logits.size(-1))\n",
    "    \n",
    "    target = trg_end.view(-1)\n",
    "\n",
    "    batch_loss = F.cross_entropy(predictions, target, \n",
    "                                 ignore_index = model.vocab.word2index[\"<PAD>\"])\n",
    "\n",
    "    reconstruction_loss = lamda*rcl\n",
    "    \n",
    "    ################# Next Part of Conversation ########################\n",
    "    \n",
    "    out_vecs, trg_end, rcl, rcli = model.teacher_forcing(whatmyname, yournameis)\n",
    "    \n",
    "    vocab_logits = model.vocab.emb2vocab(out_vecs)\n",
    "\n",
    "    predictions = vocab_logits.view(-1, vocab_logits.size(-1))\n",
    "    \n",
    "    target = trg_end.view(-1)\n",
    "    \n",
    "    acc = accuracy_score(target, torch.argmax(predictions, dim=1))\n",
    "\n",
    "    batch_loss += F.cross_entropy(predictions, target, \n",
    "                                 ignore_index = model.vocab.word2index[\"<PAD>\"])\n",
    "    \n",
    "    reconstruction_loss += lamda*rcl\n",
    "    \n",
    "    ################ Cobine Losses +++++++++++++++++++++++++++++\n",
    "    conversation_loss = batch_loss + reconstruction_loss\n",
    "    \n",
    "    scheduler.step(conversation_loss)\n",
    "    optimizer.zero_grad()\n",
    "    conversation_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if batch % int(total_batches/20 + 1) == 0:\n",
    "        \n",
    "        loss_all_list.append(conversation_loss.float().item())\n",
    "        rcloss_all_list.append(reconstruction_loss.float().item())\n",
    "        accuracy_list.append(acc)\n",
    "        mean_accuracy = np.mean(accuracy_list[-10:])\n",
    "\n",
    "        if mean_accuracy > best_acc:\n",
    "            print('Saving Model...')\n",
    "            best_acc = mean_accuracy\n",
    "            \n",
    "            ''' \n",
    "            pickle.dump(model.vocab.word2index,open(\"modelstate/word2index.p\",\"wb\"))\n",
    "            pickle.dump(model.vocab.index2word,open(\"modelstate/index2word.p\",\"wb\"))\n",
    "            pickle.dump(model.vocab.emb2vocab.weight,open(\"modelstate/emb2vocab.weight.p\",\"wb\"))\n",
    "            pickle.dump(model.vocab.embedding.weight,open(\"modelstate/embedding.weight.p\",\"wb\"))\n",
    "            pickle.dump(model.context_vec,open(\"modelstate/context_vec.p\",\"wb\"))\n",
    "            pickle.dump(model.mnm.memfunc.Ws,open(\"modelstate/Ws.p\",\"wb\"))\n",
    "            save_model(model,\"modelstate/task.pth\")\n",
    "            '''\n",
    "            \n",
    "        print(\"mean accuracy\", round(mean_accuracy,4), \n",
    "              \"celoss\", round(batch_loss.float().item(),4), \n",
    "              \"rcloss\", round(reconstruction_loss.float().item(),6), \n",
    "              \"d_rcloss\", round((rcli - rcl).float().item(),4),\n",
    "              \"training progress\", round(batch/total_batches,4),\n",
    "              \"learning rate\", scheduler._last_lr)\n",
    "            \n",
    "        if mean_accuracy > 0.97:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
